NumPy User Guide
Release 1.11.0 Written by the NumPy community
May 29, 2016

CONTENTS

1 Setting up 1.1 What is NumPy? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Installing NumPy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 3 4

2 Quickstart tutorial 2.1 Prerequisites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 The Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Shape Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Copies and Views . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.5 Less Basic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.6 Fancy indexing and index tricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.7 Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.8 Tricks and Tips . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.9 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 5 5 13 15 17 18 23 24 26

3 Numpy basics 3.1 Data types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Array creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 I/O with Numpy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Indexing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5 Broadcasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.6 Byte-swapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.7 Structured arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.8 Subclassing ndarray . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

27 27 29 32 39 45 48 50 54

4 Miscellaneous 4.1 IEEE 754 Floating Point Special Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 How numpy handles numerical exceptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Interfacing to C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Interfacing to Fortran: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.6 Interfacing to C++: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

63 63 64 64 64 66 66

5 Numpy for Matlab users 5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Some Key Differences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 ‘array’ or ‘matrix’? Which should I use? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Facilities for Matrix Users . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.5 Table of Rough MATLAB-NumPy Equivalents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.7 Customizing Your Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

69 69 69 69 71 71 74 75

i

5.8 Links . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75

6 Building from source 6.1 Prerequisites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Basic Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 FORTRAN ABI mismatch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4 Disabling ATLAS and other accelerated libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.5 Supplying additional compiler ﬂags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.6 Building with ATLAS support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

77 77 77 78 78 79 79

7 Using Numpy C-API

81

7.1 How to extend NumPy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

7.2 Using Python as glue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88

7.3 Writing your own ufunc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104

7.4 Beyond the Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122

Index

131

ii

NumPy User Guide, Release 1.11.0
This guide is intended as an introductory overview of NumPy and explains how to install and make use of the most important features of NumPy. For detailed reference documentation of the functions and classes contained in the package, see the reference.

CONTENTS

1

NumPy User Guide, Release 1.11.0 2 CONTENTS

CHAPTER
ONE
SETTING UP
1.1 What is NumPy?
NumPy is the fundamental package for scientiﬁc computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. At the core of the NumPy package, is the ndarray object. This encapsulates n-dimensional arrays of homogeneous data types, with many operations being performed in compiled code for performance. There are several important differences between NumPy arrays and the standard Python sequences:
• NumPy arrays have a ﬁxed size at creation, unlike Python lists (which can grow dynamically). Changing the size of an ndarray will create a new array and delete the original.
• The elements in a NumPy array are all required to be of the same data type, and thus will be the same size in memory. The exception: one can have arrays of (Python, including NumPy) objects, thereby allowing for arrays of different sized elements.
• NumPy arrays facilitate advanced mathematical and other types of operations on large numbers of data. Typically, such operations are executed more efﬁciently and with less code than is possible using Python’s built-in sequences.
• A growing plethora of scientiﬁc and mathematical Python-based packages are using NumPy arrays; though these typically support Python-sequence input, they convert such input to NumPy arrays prior to processing, and they often output NumPy arrays. In other words, in order to efﬁciently use much (perhaps even most) of today’s scientiﬁc/mathematical Python-based software, just knowing how to use Python’s built-in sequence types is insufﬁcient - one also needs to know how to use NumPy arrays.
The points about sequence size and speed are particularly important in scientiﬁc computing. As a simple example, consider the case of multiplying each element in a 1-D sequence with the corresponding element in another sequence of the same length. If the data are stored in two Python lists, a and b, we could iterate over each element: c = [] for i in range(len(a)):
c.append(a[i]*b[i])
This produces the correct answer, but if a and b each contain millions of numbers, we will pay the price for the inefﬁciencies of looping in Python. We could accomplish the same task much more quickly in C by writing (for clarity we neglect variable declarations and initializations, memory allocation, etc.) for (i = 0; i < rows; i++): {
c[i] = a[i]*b[i]; }
3

NumPy User Guide, Release 1.11.0
This saves all the overhead involved in interpreting the Python code and manipulating Python objects, but at the expense of the beneﬁts gained from coding in Python. Furthermore, the coding work required increases with the dimensionality of our data. In the case of a 2-D array, for example, the C code (abridged as before) expands to for (i = 0; i < rows; i++): {
for (j = 0; j < columns; j++): { c[i][j] = a[i][j]*b[i][j];
} }
NumPy gives us the best of both worlds: element-by-element operations are the “default mode” when an ndarray is involved, but the element-by-element operation is speedily executed by pre-compiled C code. In NumPy c=a*b
does what the earlier examples do, at near-C speeds, but with the code simplicity we expect from something based on Python. Indeed, the NumPy idiom is even simpler! This last example illustrates two of NumPy’s features which are the basis of much of its power: vectorization and broadcasting. Vectorization describes the absence of any explicit looping, indexing, etc., in the code - these things are taking place, of course, just “behind the scenes” in optimized, pre-compiled C code. Vectorized code has many advantages, among which are:
• vectorized code is more concise and easier to read • fewer lines of code generally means fewer bugs • the code more closely resembles standard mathematical notation (making it easier, typically, to correctly code
mathematical constructs) • vectorization results in more “Pythonic” code. Without vectorization, our code would be littered with inefﬁcient
and difﬁcult to read for loops. Broadcasting is the term used to describe the implicit element-by-element behavior of operations; generally speaking, in NumPy all operations, not just arithmetic operations, but logical, bit-wise, functional, etc., behave in this implicit element-by-element fashion, i.e., they broadcast. Moreover, in the example above, a and b could be multidimensional arrays of the same shape, or a scalar and an array, or even two arrays of with different shapes, provided that the smaller array is “expandable” to the shape of the larger in such a way that the resulting broadcast is unambiguous. For detailed “rules” of broadcasting see numpy.doc.broadcasting. NumPy fully supports an object-oriented approach, starting, once again, with ndarray. For example, ndarray is a class, possessing numerous methods and attributes. Many of its methods mirror functions in the outer-most NumPy namespace, giving the programmer complete freedom to code in whichever paradigm she prefers and/or which seems most appropriate to the task at hand.
1.2 Installing NumPy
In most use cases the best way to install NumPy on your system is by using an pre-built package for your operating system. Please see http://scipy.org/install.html for links to available options. For instructions on building for source package, see Building from source. This information is useful mainly for advanced users.
4 Chapter 1. Setting up

CHAPTER
TWO
QUICKSTART TUTORIAL
2.1 Prerequisites
Before reading this tutorial you should know a bit of Python. If you would like to refresh your memory, take a look at the Python tutorial. If you wish to work the examples in this tutorial, you must also have some software installed on your computer. Please see http://scipy.org/install.html for instructions.
2.2 The Basics
NumPy’s main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type, indexed by a tuple of positive integers. In Numpy dimensions are called axes. The number of axes is rank. For example, the coordinates of a point in 3D space [1, 2, 1] is an array of rank 1, because it has one axis. That axis has a length of 3. In example pictured below, the array has rank 2 (it is 2-dimensional). The ﬁrst dimension (axis) has a length of 2, the second dimension has a length of 3. [[ 1., 0., 0.],
[ 0., 1., 2.]]
Numpy’s array class is called ndarray. It is also known by the alias array. Note that numpy.array is not the same as the Standard Python Library class array.array, which only handles one-dimensional arrays and offers less functionality. The more important attributes of an ndarray object are: ndarray.ndim
the number of axes (dimensions) of the array. In the Python world, the number of dimensions is referred to as rank. ndarray.shape the dimensions of the array. This is a tuple of integers indicating the size of the array in each dimension. For a matrix with n rows and m columns, shape will be (n,m). The length of the shape tuple is therefore the rank, or number of dimensions, ndim. ndarray.size the total number of elements of the array. This is equal to the product of the elements of shape. ndarray.dtype an object describing the type of the elements in the array. One can create or specify dtype’s using standard Python types. Additionally NumPy provides types of its own. numpy.int32, numpy.int16, and numpy.ﬂoat64 are some examples.
5

NumPy User Guide, Release 1.11.0
ndarray.itemsize the size in bytes of each element of the array. For example, an array of elements of type float64 has itemsize 8 (=64/8), while one of type complex32 has itemsize 4 (=32/8). It is equivalent to ndarray.dtype.itemsize.
ndarray.data the buffer containing the actual elements of the array. Normally, we won’t need to use this attribute because we will access the elements in an array using indexing facilities.
2.2.1 An example
>>> import numpy as np >>> a = np.arange(15).reshape(3, 5) >>> a array([[ 0, 1, 2, 3, 4],
[ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) >>> a.shape (3, 5) >>> a.ndim 2 >>> a.dtype.name 'int64' >>> a.itemsize 8 >>> a.size 15 >>> type(a) <type 'numpy.ndarray'> >>> b = np.array([6, 7, 8]) >>> b array([6, 7, 8]) >>> type(b) <type 'numpy.ndarray'>
2.2.2 Array Creation
There are several ways to create arrays.
For example, you can create an array from a regular Python list or tuple using the array function. The type of the resulting array is deduced from the type of the elements in the sequences.
>>> import numpy as np >>> a = np.array([2,3,4]) >>> a array([2, 3, 4]) >>> a.dtype dtype('int64') >>> b = np.array([1.2, 3.5, 5.1]) >>> b.dtype dtype('float64')
A frequent error consists in calling array with multiple numeric arguments, rather than providing a single list of numbers as an argument.
6 Chapter 2. Quickstart tutorial

NumPy User Guide, Release 1.11.0

>>> a = np.array(1,2,3,4) # WRONG >>> a = np.array([1,2,3,4]) # RIGHT

array transforms sequences of sequences into two-dimensional arrays, sequences of sequences of sequences into three-dimensional arrays, and so on.
>>> b = np.array([(1.5,2,3), (4,5,6)]) >>> b array([[ 1.5, 2. , 3. ],
[ 4. , 5. , 6. ]])

The type of the array can also be explicitly speciﬁed at creation time:
>>> c = np.array( [ [1,2], [3,4] ], dtype=complex ) >>> c array([[ 1.+0.j, 2.+0.j],
[ 3.+0.j, 4.+0.j]])

Often, the elements of an array are originally unknown, but its size is known. Hence, NumPy offers several functions to create arrays with initial placeholder content. These minimize the necessity of growing arrays, an expensive operation.

The function zeros creates an array full of zeros, the function ones creates an array full of ones, and the function empty creates an array whose initial content is random and depends on the state of the memory. By default, the dtype of the created array is float64.

>>> np.zeros( (3,4) ) array([[ 0., 0., 0., 0.],
[ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) >>> np.ones( (2,3,4), dtype=np.int16 ) array([[[ 1, 1, 1, 1],
[ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16) >>> np.empty( (2,3) ) array([[ 3.73603959e-262, 6.02658058e-154, [ 5.30498948e-313, 3.14673309e-307,

# dtype can also be specified
# uninitialized, output may vary 6.55490914e-260], 1.00000000e+000]])

To create sequences of numbers, NumPy provides a function analogous to range that returns arrays instead of lists

>>> np.arange( 10, 30, 5 ) array([10, 15, 20, 25]) >>> np.arange( 0, 2, 0.3 ) array([ 0. , 0.3, 0.6, 0.9,

1.2,

# it accepts float arguments 1.5, 1.8])

When arange is used with ﬂoating point arguments, it is generally not possible to predict the number of elements obtained, due to the ﬁnite ﬂoating point precision. For this reason, it is usually better to use the function linspace that receives as an argument the number of elements that we want, instead of the step:

>>> from numpy import pi

>>> np.linspace( 0, 2, 9 )

# 9 numbers from 0 to 2

array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ])

>>> x = np.linspace( 0, 2*pi, 100 )

# useful to evaluate function at lots of points

>>> f = np.sin(x)

See also:

2.2. The Basics

7

NumPy User Guide, Release 1.11.0

array, zeros, zeros_like, ones, ones_like, empty, empty_like, arange, linspace, numpy.random.rand, numpy.random.randn, fromfunction, fromfile

2.2.3 Printing Arrays

When you print an array, NumPy displays it in a similar way to nested lists, but with the following layout:

• the last axis is printed from left to right,

• the second-to-last is printed from top to bottom,

• the rest are also printed from top to bottom, with each slice separated from the next by an empty line.

One-dimensional arrays are then printed as rows, bidimensionals as matrices and tridimensionals as lists of matrices.

>>> a = np.arange(6) >>> print(a) [0 1 2 3 4 5] >>> >>> b = np.arange(12).reshape(4,3) >>> print(b) [[ 0 1 2]
[ 3 4 5] [ 6 7 8] [ 9 10 11]] >>> >>> c = np.arange(24).reshape(2,3,4) >>> print(c) [[[ 0 1 2 3]
[ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]]

# 1d array # 2d array
# 3d array

See below to get more details on reshape.
If an array is too large to be printed, NumPy automatically skips the central part of the array and only prints the corners:
>>> print(np.arange(10000)) [ 0 1 2 ..., 9997 9998 9999] >>> >>> print(np.arange(10000).reshape(100,100)) [[ 0 1 2 ..., 97 98 99]
[ 100 101 102 ..., 197 198 199] [ 200 201 202 ..., 297 298 299] ..., [9700 9701 9702 ..., 9797 9798 9799] [9800 9801 9802 ..., 9897 9898 9899] [9900 9901 9902 ..., 9997 9998 9999]]

To disable this behaviour and force NumPy to print the entire array, you can change the printing options using set_printoptions.
>>> np.set_printoptions(threshold='nan')

8 Chapter 2. Quickstart tutorial

NumPy User Guide, Release 1.11.0

2.2.4 Basic Operations

Arithmetic operators on arrays apply elementwise. A new array is created and ﬁlled with the result.
>>> a = np.array( [20,30,40,50] ) >>> b = np.arange( 4 ) >>> b array([0, 1, 2, 3]) >>> c = a-b >>> c array([20, 29, 38, 47]) >>> b**2 array([0, 1, 4, 9]) >>> 10*np.sin(a) array([ 9.12945251, -9.88031624, 7.4511316 , -2.62374854]) >>> a<35 array([ True, True, False, False], dtype=bool)

Unlike in many matrix languages, the product operator * operates elementwise in NumPy arrays. The matrix product can be performed using the dot function or method:

>>> A = np.array( [[1,1], ... [0,1]] ) >>> B = np.array( [[2,0], ... [3,4]] ) >>> A*B array([[2, 0],
[0, 4]]) >>> A.dot(B) array([[5, 4],
[3, 4]]) >>> np.dot(A, B) array([[5, 4],
[3, 4]])

# elementwise product # matrix product # another matrix product

Some operations, such as += and *=, act in place to modify an existing array rather than create a new one.

>>> a = np.ones((2,3), dtype=int)

>>> b = np.random.random((2,3))

>>> a *= 3 >>> a

array([[3, 3, 3],

[3, 3, 3]])

>>> b += a

>>> b

array([[ 3.417022 , 3.72032449, 3.00011437],

[ 3.30233257, 3.14675589, 3.09233859]])

>>> a += b

# b is not automatically converted to integer type

Traceback (most recent call last):

...

TypeError: Cannot cast ufunc add output from dtype('float64') to dtype('int64') with casting rule 'sa

When operating with arrays of different types, the type of the resulting array corresponds to the more general or precise one (a behavior known as upcasting).
>>> a = np.ones(3, dtype=np.int32) >>> b = np.linspace(0,pi,3) >>> b.dtype.name 'float64'

2.2. The Basics

9

NumPy User Guide, Release 1.11.0

>>> c = a+b

>>> c

array([ 1.

, 2.57079633, 4.14159265])

>>> c.dtype.name

'float64'

>>> d = np.exp(c*1j)

>>> d

array([ 0.54030231+0.84147098j, -0.84147098+0.54030231j,

-0.54030231-0.84147098j])

>>> d.dtype.name

'complex128'

Many unary operations, such as computing the sum of all the elements in the array, are implemented as methods of the ndarray class.

>>> a = np.random.random((2,3)) >>> a array([[ 0.18626021, 0.34556073,
[ 0.53881673, 0.41919451, >>> a.sum() 2.5718191614547998 >>> a.min() 0.1862602113776709 >>> a.max() 0.6852195003967595

0.39676747], 0.6852195 ]])

By default, these operations apply to the array as though it were a list of numbers, regardless of its shape. However, by specifying the axis parameter you can apply an operation along the speciﬁed axis of an array:

>>> b = np.arange(12).reshape(3,4) >>> b array([[ 0, 1, 2, 3],
[ 4, 5, 6, 7], [ 8, 9, 10, 11]]) >>> >>> b.sum(axis=0) array([12, 15, 18, 21]) >>> >>> b.min(axis=1) array([0, 4, 8]) >>> >>> b.cumsum(axis=1) array([[ 0, 1, 3, 6], [ 4, 9, 15, 22], [ 8, 17, 27, 38]])

# sum of each column # min of each row # cumulative sum along each row

2.2.5 Universal Functions

NumPy provides familiar mathematical functions such as sin, cos, and exp. In NumPy, these are called “universal functions”(ufunc). Within NumPy, these functions operate elementwise on an array, producing an array as output.

>>> B = np.arange(3)

>>> B

array([0, 1, 2])

>>> np.exp(B)

array([ 1.

, 2.71828183,

>>> np.sqrt(B)

7.3890561 ])

10 Chapter 2. Quickstart tutorial

NumPy User Guide, Release 1.11.0

array([ 0.

, 1.

,

>>> C = np.array([2., -1., 4.])

>>> np.add(B, C)

array([ 2., 0., 6.])

1.41421356])

See also:
all, any, apply_along_axis, argmax, argmin, argsort, average, bincount, ceil, clip, conj, corrcoef, cov, cross, cumprod, cumsum, diff, dot, floor, inner, inv, lexsort, max, maximum, mean, median, min, minimum, nonzero, outer, prod, re, round, sort, std, sum, trace, transpose, var, vdot, vectorize, where

2.2.6 Indexing, Slicing and Iterating

One-dimensional arrays can be indexed, sliced and iterated over, much like lists and other Python sequences.

>>> a = np.arange(10)**3 >>> a

array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729])

>>> a[2]

8

>>> a[2:5]

array([ 8, 27, 64])

>>> a[:6:2] = -1000 # equivalent to a[0:6:2] = -1000; from start to position 6, exclusive, set eve

>>> a

array([-1000,

1, -1000, 27, -1000, 125, 216, 343, 512, 729])

>>> a[ : :-1]

# reversed a

array([ 729, 512, 343, 216, 125, -1000, 27, -1000,

1, -1000])

>>> for i in a:

... print(i**(1/3.)) ...

nan

1.0

nan

3.0

nan

5.0

6.0

7.0

8.0

9.0

Multidimensional arrays can have one index per axis. These indices are given in a tuple separated by commas:

>>> def f(x,y):

... return 10*x+y

...

>>> b = np.fromfunction(f,(5,4),dtype=int)

>>> b

array([[ 0, 1, 2, 3],

[10, 11, 12, 13],

[20, 21, 22, 23],

[30, 31, 32, 33],

[40, 41, 42, 43]])

>>> b[2,3]

23

>>> b[0:5, 1]

# each row in the second column of b

array([ 1, 11, 21, 31, 41])

2.2. The Basics

11

NumPy User Guide, Release 1.11.0

>>> b[ : ,1] array([ 1, 11, 21, 31, 41]) >>> b[1:3, : ] array([[10, 11, 12, 13],
[20, 21, 22, 23]])

# equivalent to the previous example # each column in the second and third row of b

When fewer indices are provided than the number of axes, the missing indices are considered complete slices:

>>> b[-1] array([40, 41, 42, 43])

# the last row. Equivalent to b[-1,:]

The expression within brackets in b[i] is treated as an i followed by as many instances of : as needed to represent the remaining axes. NumPy also allows you to write this using dots as b[i,...].

The dots (...) represent as many colons as needed to produce a complete indexing tuple. For example, if x is a rank 5 array (i.e., it has 5 axes), then

• x[1,2,...] is equivalent to x[1,2,:,:,:],

• x[...,3] to x[:,:,:,:,3] and

• x[4,...,5,:] to x[4,:,:,5,:].

>>> c = np.array( [[[ 0, 1, 2], ... [ 10, 12, 13]], ... [[100,101,102], ... [110,112,113]]]) >>> c.shape (2, 2, 3) >>> c[1,...] array([[100, 101, 102],
[110, 112, 113]]) >>> c[...,2] array([[ 2, 13],
[102, 113]])

# a 3D array (two stacked 2D arrays)
# same as c[1,:,:] or c[1] # same as c[:,:,2]

Iterating over multidimensional arrays is done with respect to the ﬁrst axis:
>>> for row in b: ... print(row) ... [0 1 2 3] [10 11 12 13] [20 21 22 23] [30 31 32 33] [40 41 42 43]

However, if one wants to perform an operation on each element in the array, one can use the flat attribute which is an iterator over all the elements of the array:
>>> for element in b.flat: ... print(element) ... 0 1 2 3 10 11 12 13

12 Chapter 2. Quickstart tutorial

NumPy User Guide, Release 1.11.0

20 21 22 23 30 31 32 33 40 41 42 43
See also:
Indexing, arrays.indexing (reference), newaxis, ndenumerate, indices

2.3 Shape Manipulation

2.3.1 Changing the shape of an array

An array has a shape given by the number of elements along each axis:
>>> a = np.floor(10*np.random.random((3,4))) >>> a array([[ 2., 8., 0., 6.],
[ 4., 5., 1., 1.], [ 8., 9., 3., 6.]]) >>> a.shape (3, 4)

The shape of an array can be changed with various commands:
>>> a.ravel() # flatten the array array([ 2., 8., 0., 6., 4., 5., 1., 1., 8., 9., 3., 6.]) >>> a.shape = (6, 2) >>> a.T array([[ 2., 0., 4., 1., 8., 3.],
[ 8., 6., 5., 1., 9., 6.]])

The order of the elements in the array resulting from ravel() is normally “C-style”, that is, the rightmost index “changes the fastest”, so the element after a[0,0] is a[0,1]. If the array is reshaped to some other shape, again the array is treated as “C-style”. Numpy normally creates arrays stored in this order, so ravel() will usually not need to copy its argument, but if the array was made by taking slices of another array or created with unusual options, it may need to be copied. The functions ravel() and reshape() can also be instructed, using an optional argument, to use FORTRAN-style arrays, in which the leftmost index changes the fastest.

The reshape function returns its argument with a modiﬁed shape, whereas the ndarray.resize method modiﬁes the array itself:

>>> a array([[ 2.,
[ 0., [ 4., [ 1., [ 8., [ 3.,

8.], 6.], 5.], 1.], 9.], 6.]])

2.3. Shape Manipulation

13

NumPy User Guide, Release 1.11.0

>>> a.resize((2,6)) >>> a array([[ 2., 8., 0., 6., 4., 5.],
[ 1., 1., 8., 9., 3., 6.]])

If a dimension is given as -1 in a reshaping operation, the other dimensions are automatically calculated:

>>> a.reshape(3,-1) array([[ 2., 8., 0.,
[ 4., 5., 1., [ 8., 9., 3.,

6.], 1.], 6.]])

See also: ndarray.shape, reshape, resize, ravel

2.3.2 Stacking together different arrays
Several arrays can be stacked together along different axes:
>>> a = np.floor(10*np.random.random((2,2))) >>> a array([[ 8., 8.],
[ 0., 0.]]) >>> b = np.floor(10*np.random.random((2,2))) >>> b array([[ 1., 8.],
[ 0., 4.]]) >>> np.vstack((a,b)) array([[ 8., 8.],
[ 0., 0.], [ 1., 8.], [ 0., 4.]]) >>> np.hstack((a,b)) array([[ 8., 8., 1., 8.], [ 0., 0., 0., 4.]])
The function column_stack stacks 1D arrays as columns into a 2D array. It is equivalent to vstack only for 1D arrays:
>>> from numpy import newaxis >>> np.column_stack((a,b)) # With 2D arrays array([[ 8., 8., 1., 8.],
[ 0., 0., 0., 4.]]) >>> a = np.array([4.,2.]) >>> b = np.array([2.,8.]) >>> a[:,newaxis] # This allows to have a 2D columns vector array([[ 4.],
[ 2.]]) >>> np.column_stack((a[:,newaxis],b[:,newaxis])) array([[ 4., 2.],
[ 2., 8.]]) >>> np.vstack((a[:,newaxis],b[:,newaxis])) # The behavior of vstack is different array([[ 4.],
[ 2.], [ 2.], [ 8.]])

14 Chapter 2. Quickstart tutorial

NumPy User Guide, Release 1.11.0

For arrays of with more than two dimensions, hstack stacks along their second axes, vstack stacks along their ﬁrst axes, and concatenate allows for an optional arguments giving the number of the axis along which the concatenation should happen.
Note
In complex cases, r_ and c_ are useful for creating arrays by stacking numbers along one axis. They allow the use of range literals (”:”) :
>>> np.r_[1:4,0,4] array([1, 2, 3, 0, 4])
When used with arrays as arguments, r_ and c_ are similar to vstack and hstack in their default behavior, but allow for an optional argument giving the number of the axis along which to concatenate.
See also:
hstack, vstack, column_stack, concatenate, c_, r_
2.3.3 Splitting one array into several smaller ones
Using hsplit, you can split an array along its horizontal axis, either by specifying the number of equally shaped arrays to return, or by specifying the columns after which the division should occur:
>>> a = np.floor(10*np.random.random((2,12))) >>> a array([[ 9., 5., 6., 3., 6., 8., 0., 7., 9., 7., 2., 7.],
[ 1., 4., 9., 2., 2., 1., 0., 6., 2., 2., 4., 0.]]) >>> np.hsplit(a,3) # Split a into 3 [array([[ 9., 5., 6., 3.],
[ 1., 4., 9., 2.]]), array([[ 6., 8., 0., 7.], [ 2., 1., 0., 6.]]), array([[ 9., 7., 2., 7.], [ 2., 2., 4., 0.]])] >>> np.hsplit(a,(3,4)) # Split a after the third and the fourth column [array([[ 9., 5., 6.], [ 1., 4., 9.]]), array([[ 3.], [ 2.]]), array([[ 6., 8., 0., 7., 9., 7., 2., 7.], [ 2., 1., 0., 6., 2., 2., 4., 0.]])]
vsplit splits along the vertical axis, and array_split allows one to specify along which axis to split.

2.4 Copies and Views
When operating and manipulating arrays, their data is sometimes copied into a new array and sometimes not. This is often a source of confusion for beginners. There are three cases:

2.4.1 No Copy at All

Simple assignments make no copy of array objects or of their data.

>>> a = np.arange(12)

>>> b = a

# no new object is created

>>> b is a

# a and b are two names for the same ndarray object

True

>>> b.shape = 3,4 # changes the shape of a

2.4. Copies and Views

15

NumPy User Guide, Release 1.11.0

>>> a.shape (3, 4)

Python passes mutable objects as references, so function calls make no copy.

>>> def f(x): ... print(id(x)) ... >>> id(a) 148293216 >>> f(a) 148293216

# id is a unique identifier of an object

2.4.2 View or Shallow Copy

Different array objects can share the same data. The view method creates a new array object that looks at the same data.

>>> c = a.view() >>> c is a False >>> c.base is a True >>> c.flags.owndata False >>> >>> c.shape = 2,6 >>> a.shape (3, 4) >>> c[0,4] = 1234 >>> a array([[ 0, 1, 2, 3],
[1234, 5, 6, 7], [ 8, 9, 10, 11]])

# c is a view of the data owned by a
# a's shape doesn't change # a's data changes

Slicing an array returns a view of it:

>>> s = a[ : , 1:3] >>> s[:] = 10 >>> a array([[ 0, 10,
[1234, 10, [ 8, 10,

# spaces added for clarity; could also be written "s = a[:,1:3]" # s[:] is a view of s. Note the difference between s=10 and s[:]=10
10, 3], 10, 7], 10, 11]])

2.4.3 Deep Copy

The copy method makes a complete copy of the array and its data.

>>> d = a.copy() >>> d is a False >>> d.base is a False >>> d[0,0] = 9999 >>> a array([[ 0, 10,

10,

3],

# a new array object with new data is created # d doesn't share anything with a

16 Chapter 2. Quickstart tutorial

NumPy User Guide, Release 1.11.0
[1234, 10, 10, 7], [ 8, 10, 10, 11]])
2.4.4 Functions and Methods Overview
Here is a list of some useful NumPy functions and methods names ordered in categories. See routines for the full list. Array Creation
arange, array, copy, empty, empty_like, eye, fromfile, fromfunction, identity, linspace, logspace, mgrid, ogrid, ones, ones_like, r, zeros, zeros_like Conversions ndarray.astype, atleast_1d, atleast_2d, atleast_3d, mat Manipulations array_split, column_stack, concatenate, diagonal, dsplit, dstack, hsplit, hstack, ndarray.item, newaxis, ravel, repeat, reshape, resize, squeeze, swapaxes, take, transpose, vsplit, vstack Questions all, any, nonzero, where Ordering argmax, argmin, argsort, max, min, ptp, searchsorted, sort Operations choose, compress, cumprod, cumsum, inner, ndarray.fill, imag, prod, put, putmask, real, sum Basic Statistics cov, mean, std, var Basic Linear Algebra cross, dot, outer, linalg.svd, vdot
2.5 Less Basic
2.5.1 Broadcasting rules
Broadcasting allows universal functions to deal in a meaningful way with inputs that do not have exactly the same shape. The ﬁrst rule of broadcasting is that if all input arrays do not have the same number of dimensions, a “1” will be repeatedly prepended to the shapes of the smaller arrays until all the arrays have the same number of dimensions. The second rule of broadcasting ensures that arrays with a size of 1 along a particular dimension act as if they had the size of the array with the largest shape along that dimension. The value of the array element is assumed to be the same along that dimension for the “broadcast” array. After application of the broadcasting rules, the sizes of all arrays must match. More details can be found in Broadcasting.

2.5. Less Basic

17

NumPy User Guide, Release 1.11.0

2.6 Fancy indexing and index tricks
NumPy offers more indexing facilities than regular Python sequences. In addition to indexing by integers and slices, as we saw before, arrays can be indexed by arrays of integers and arrays of booleans.

2.6.1 Indexing with Arrays of Indices

>>> a = np.arange(12)**2 >>> i = np.array( [ 1,1,3,8,5 ] ) >>> a[i] array([ 1, 1, 9, 64, 25]) >>> >>> j = np.array( [ [ 3, 4], [ 9, 7 ] ] ) >>> a[j] array([[ 9, 16],
[81, 49]])

# the first 12 square numbers # an array of indices # the elements of a at the positions i
# a bidimensional array of indices # the same shape as j

When the indexed array a is multidimensional, a single array of indices refers to the ﬁrst dimension of a. The following example shows this behavior by converting an image of labels into a color image using a palette.

>>> palette = np.array( [ [0,0,0], ... [255,0,0], ... [0,255,0], ... [0,0,255], ... [255,255,255] ] ) >>> image = np.array( [ [ 0, 1, 2, 0 ], ... [ 0, 3, 4, 0 ] ] ) >>> palette[image] array([[[ 0, 0, 0],
[255, 0, 0], [ 0, 255, 0], [ 0, 0, 0]], [[ 0, 0, 0], [ 0, 0, 255], [255, 255, 255], [ 0, 0, 0]]])

# black # red # green # blue # white # each value corresponds to a color in the palette
# the (2,4,3) color image

We can also give indexes for more than one dimension. The arrays of indices for each dimension must have the same shape.

>>> a = np.arange(12).reshape(3,4) >>> a array([[ 0, 1, 2, 3],
[ 4, 5, 6, 7], [ 8, 9, 10, 11]]) >>> i = np.array( [ [0,1], ... [1,2] ] ) >>> j = np.array( [ [2,1], ... [3,3] ] ) >>> >>> a[i,j] array([[ 2, 5], [ 7, 11]]) >>> >>> a[i,2] array([[ 2, 6], [ 6, 10]])

# indices for the first dim of a # indices for the second dim
# i and j must have equal shape

18 Chapter 2. Quickstart tutorial

NumPy User Guide, Release 1.11.0

>>> >>> a[:,j] array([[[ 2, 1],
[ 3, 3]], [[ 6, 5],
[ 7, 7]], [[10, 9],
[11, 11]]])

# i.e., a[ : , j]

Naturally, we can put i and j in a sequence (say a list) and then do the indexing with the list.

>>> l = [i,j] >>> a[l] array([[ 2, 5],
[ 7, 11]])

# equivalent to a[i,j]

However, we can not do this by putting i and j into an array, because this array will be interpreted as indexing the ﬁrst dimension of a.

>>> s = np.array( [i,j] )

>>> a[s]

# not what we want

Traceback (most recent call last):

File "<stdin>", line 1, in ?

IndexError: index (3) out of range (0<=index<=2) in dimension 0

>>>

>>> a[tuple(s)]

# same as a[i,j]

array([[ 2, 5],

[ 7, 11]])

Another common use of indexing with arrays is the search of the maximum value of time-dependent series :

>>> time = np.linspace(20, 145, 5)

# time scale

>>> data = np.sin(np.arange(20)).reshape(5,4)

# 4 time-dependent series

>>> time

array([ 20. , 51.25, 82.5 , 113.75, 145. ])

>>> data

array([[ 0.

, 0.84147098, 0.90929743, 0.14112001],

[-0.7568025 , -0.95892427, -0.2794155 , 0.6569866 ],

[ 0.98935825, 0.41211849, -0.54402111, -0.99999021],

[-0.53657292, 0.42016704, 0.99060736, 0.65028784],

[-0.28790332, -0.96139749, -0.75098725, 0.14987721]])

>>>

>>> ind = data.argmax(axis=0)

# index of the maxima for each series

>>> ind

array([2, 0, 3, 1])

>>>

>>> time_max = time[ ind]

# times corresponding to the maxima

>>>

>>> data_max = data[ind, xrange(data.shape[1])] # => data[ind[0],0], data[ind[1],1]...

>>>

>>> time_max

array([ 82.5 , 20. , 113.75, 51.25])

>>> data_max

array([ 0.98935825, 0.84147098, 0.99060736, 0.6569866 ])

>>>

>>> np.all(data_max == data.max(axis=0))

True

You can also use indexing with arrays as a target to assign to:

2.6. Fancy indexing and index tricks

19

NumPy User Guide, Release 1.11.0

>>> a = np.arange(5) >>> a array([0, 1, 2, 3, 4]) >>> a[[1,3,4]] = 0 >>> a array([0, 0, 2, 0, 0])
However, when the list of indices contains repetitions, the assignment is done several times, leaving behind the last value:
>>> a = np.arange(5) >>> a[[0,0,2]]=[1,2,3] >>> a array([2, 1, 3, 3, 4])
This is reasonable enough, but watch out if you want to use Python’s += construct, as it may not do what you expect:
>>> a = np.arange(5) >>> a[[0,0,2]]+=1 >>> a array([1, 1, 3, 3, 4])
Even though 0 occurs twice in the list of indices, the 0th element is only incremented once. This is because Python requires “a+=1” to be equivalent to “a=a+1”.

2.6.2 Indexing with Boolean Arrays

When we index arrays with arrays of (integer) indices we are providing the list of indices to pick. With boolean indices the approach is different; we explicitly choose which items in the array we want and which ones we don’t.

The most natural way one can think of for boolean indexing is to use boolean arrays that have the same shape as the original array:

>>> a = np.arange(12).reshape(3,4)

>>> b = a > 4

>>> b

# b is a boolean with a's shape

array([[False, False, False, False],

[False, True, True, True],

[ True, True, True, True]], dtype=bool)

>>> a[b]

# 1d array with the selected elements

array([ 5, 6, 7, 8, 9, 10, 11])

This property can be very useful in assignments:
>>> a[b] = 0 >>> a array([[0, 1, 2, 3],
[4, 0, 0, 0], [0, 0, 0, 0]])

# All elements of 'a' higher than 4 become 0

You can look at the following example to see how to use boolean indexing to generate an image of the Mandelbrot set:
>>> import numpy as np >>> import matplotlib.pyplot as plt >>> def mandelbrot( h,w, maxit=20 ): ... """Returns an image of the Mandelbrot fractal of size (h,w).""" ... y,x = np.ogrid[ -1.4:1.4:h*1j, -2:0.8:w*1j ] ... c = x+y*1j

20 Chapter 2. Quickstart tutorial

NumPy User Guide, Release 1.11.0

... z = c

... divtime = maxit + np.zeros(z.shape, dtype=int)

...

... for i in range(maxit):

... z = z**2 + c

... diverge = z*np.conj(z) > 2**2

# who is diverging

... div_now = diverge & (divtime==maxit) # who is diverging now

... divtime[div_now] = i

# note when

... z[diverge] = 2

# avoid diverging too much

...

... return divtime

>>> plt.imshow(mandelbrot(400,400))

>>> plt.show()

0 50 100 150 200 250 300 350 400 0 50 100 150 200 250 300 350 400

The second way of indexing with booleans is more similar to integer indexing; for each dimension of the array we give a 1D boolean array selecting the slices we want.

>>> a = np.arange(12).reshape(3,4) >>> b1 = np.array([False,True,True]) >>> b2 = np.array([True,False,True,False]) >>> >>> a[b1,:] array([[ 4, 5, 6, 7],
[ 8, 9, 10, 11]]) >>> >>> a[b1] array([[ 4, 5, 6, 7],
[ 8, 9, 10, 11]]) >>> >>> a[:,b2] array([[ 0, 2],
[ 4, 6], [ 8, 10]]) >>> >>> a[b1,b2] array([ 4, 10])

# first dim selection # second dim selection # selecting rows
# same thing
# selecting columns
# a weird thing to do

Note that the length of the 1D boolean array must coincide with the length of the dimension (or axis) you want to slice.

2.6. Fancy indexing and index tricks

21

NumPy User Guide, Release 1.11.0
In the previous example, b1 is a 1-rank array with length 3 (the number of rows in a), and b2 (of length 4) is suitable to index the 2nd rank (columns) of a.
2.6.3 The ix_() function
The ix_ function can be used to combine different vectors so as to obtain the result for each n-uplet. For example, if you want to compute all the a+b*c for all the triplets taken from each of the vectors a, b and c:
>>> a = np.array([2,3,4,5]) >>> b = np.array([8,5,4]) >>> c = np.array([5,4,6,8,3]) >>> ax,bx,cx = np.ix_(a,b,c) >>> ax array([[[2]],
[[3]], [[4]], [[5]]]) >>> bx array([[[8],
[5], [4]]]) >>> cx array([[[5, 4, 6, 8, 3]]]) >>> ax.shape, bx.shape, cx.shape ((4, 1, 1), (1, 3, 1), (1, 1, 5)) >>> result = ax+bx*cx >>> result array([[[42, 34, 50, 66, 26], [27, 22, 32, 42, 17], [22, 18, 26, 34, 14]], [[43, 35, 51, 67, 27], [28, 23, 33, 43, 18], [23, 19, 27, 35, 15]], [[44, 36, 52, 68, 28], [29, 24, 34, 44, 19], [24, 20, 28, 36, 16]], [[45, 37, 53, 69, 29], [30, 25, 35, 45, 20], [25, 21, 29, 37, 17]]]) >>> result[3,2,4] 17 >>> a[3]+b[2]*c[4] 17
You could also implement the reduce as follows:
>>> def ufunc_reduce(ufct, *vectors): ... vs = np.ix_(*vectors) ... r = ufct.identity ... for v in vs: ... r = ufct(r,v) ... return r
and then use it as:
>>> ufunc_reduce(np.add,a,b,c) array([[[15, 14, 16, 18, 13],
[12, 11, 13, 15, 10],
22 Chapter 2. Quickstart tutorial

NumPy User Guide, Release 1.11.0
[11, 10, 12, 14, 9]], [[16, 15, 17, 19, 14],
[13, 12, 14, 16, 11], [12, 11, 13, 15, 10]], [[17, 16, 18, 20, 15], [14, 13, 15, 17, 12], [13, 12, 14, 16, 11]], [[18, 17, 19, 21, 16], [15, 14, 16, 18, 13], [14, 13, 15, 17, 12]]])
The advantage of this version of reduce compared to the normal ufunc.reduce is that it makes use of the Broadcasting Rules in order to avoid creating an argument array the size of the output times the number of vectors.
2.6.4 Indexing with strings
See RecordArrays.
2.7 Linear Algebra
Work in progress. Basic linear algebra to be included here.
2.7.1 Simple Array Operations
See linalg.py in numpy folder for more. >>> import numpy as np >>> a = np.array([[1.0, 2.0], [3.0, 4.0]]) >>> print(a) [[ 1. 2.]
[ 3. 4.]]
>>> a.transpose() array([[ 1., 3.],
[ 2., 4.]])
>>> np.linalg.inv(a) array([[-2. , 1. ],
[ 1.5, -0.5]])
>>> u = np.eye(2) # unit 2x2 matrix; "eye" represents "I" >>> u array([[ 1., 0.],
[ 0., 1.]]) >>> j = np.array([[0.0, -1.0], [1.0, 0.0]])
>>> np.dot (j, j) # matrix product array([[-1., 0.],
[ 0., -1.]])
>>> np.trace(u) # trace 2.0

2.7. Linear Algebra

23

NumPy User Guide, Release 1.11.0

>>> y = np.array([[5.], [7.]]) >>> np.linalg.solve(a, y) array([[-3.],
[ 4.]])

>>> np.linalg.eig(j)

(array([ 0.+1.j, 0.-1.j]), array([[ 0.70710678+0.j

, 0.70710678-0.j

[ 0.00000000-0.70710678j, 0.00000000+0.70710678j]]))

Parameters: square matrix
Returns The eigenvalues, each repeated according to its multiplicity. The normalized (unit "length") eigenvectors, such that the column ``v[:,i]`` is the eigenvector corresponding to the eigenvalue ``w[i]`` .

],

2.8 Tricks and Tips
Here we give a list of short and useful tips.
2.8.1 “Automatic” Reshaping
To change the dimensions of an array, you can omit one of the sizes which will then be deduced automatically:
>>> a = np.arange(30) >>> a.shape = 2,-1,3 # -1 means "whatever is needed" >>> a.shape (2, 5, 3) >>> a array([[[ 0, 1, 2],
[ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]], [[15, 16, 17], [18, 19, 20], [21, 22, 23], [24, 25, 26], [27, 28, 29]]])

2.8.2 Vector Stacking

How do we construct a 2D array from a list of equally-sized row vectors? In MATLAB this is quite easy: if x and y are two vectors of the same length you only need do m=[x;y]. In NumPy this works via the functions column_stack, dstack, hstack and vstack, depending on the dimension in which the stacking is to be done. For example:

x = np.arange(0,10,2) y = np.arange(5) m = np.vstack([x,y])
xy = np.hstack([x,y])

# x=([0,2,4,6,8]) # y=([0,1,2,3,4]) # m=([[0,2,4,6,8], # [0,1,2,3,4]]) # xy =([0,2,4,6,8,0,1,2,3,4])

24 Chapter 2. Quickstart tutorial

NumPy User Guide, Release 1.11.0

The logic behind those functions in more than two dimensions can be strange. See also: Numpy for Matlab users

2.8.3 Histograms

The NumPy histogram function applied to an array returns a pair of vectors: the histogram of the array and the vector of bins. Beware: matplotlib also has a function to build histograms (called hist, as in Matlab) that differs from the one in NumPy. The main difference is that pylab.hist plots the histogram automatically, while numpy.histogram only generates the data.

>>> import numpy as np

>>> import matplotlib.pyplot as plt

>>> # Build a vector of 10000 normal deviates with variance 0.5^2 and mean 2

>>> mu, sigma = 2, 0.5

>>> v = np.random.normal(mu,sigma,10000)

>>> # Plot a normalized histogram with 50 bins

>>> plt.hist(v, bins=50, normed=1)

# matplotlib version (plot)

>>> plt.show()

0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.00.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

>>> # Compute the histogram with numpy and then plot it >>> (n, bins) = np.histogram(v, bins=50, normed=True) # NumPy version (no plot) >>> plt.plot(.5*(bins[1:]+bins[:-1]), n) >>> plt.show()

2.8. Tricks and Tips

25

NumPy User Guide, Release 1.11.0
0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.00.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
2.9 Further reading
• The Python tutorial • reference • SciPy Tutorial • SciPy Lecture Notes • A matlab, R, IDL, NumPy/SciPy dictionary
26 Chapter 2. Quickstart tutorial

CHAPTER
THREE
NUMPY BASICS

3.1 Data types
See also: Data type objects

3.1.1 Array types and conversions between types

Numpy supports a much greater variety of numerical types than Python does. This section shows which are available, and how to modify an array’s data-type.

Data type
bool_
int_ intc intp int8 int16 int32 int64 uint8 uint16 uint32 uint64
ﬂoat_ ﬂoat16 ﬂoat32 ﬂoat64
complex_ complex64 complex128

Description Boolean (True or False) stored as a byte
Default integer type (same as C long; normally either int64 or int32)
Identical to C int (normally int32 or int64) Integer used for indexing (same as C ssize_t; normally either int32 or int64) Byte (-128 to 127) Integer (-32768 to 32767) Integer (-2147483648 to 2147483647) Integer (-9223372036854775808 to 9223372036854775807) Unsigned integer (0 to 255) Unsigned integer (0 to 65535) Unsigned integer (0 to 4294967295) Unsigned integer (0 to 18446744073709551615) Shorthand for float64.
Half precision ﬂoat: sign bit, 5 bits exponent, 10 bits mantissa Single precision ﬂoat: sign bit, 8 bits exponent, 23 bits mantissa Double precision ﬂoat: sign bit, 11 bits exponent, 52 bits mantissa Shorthand for complex128.
Complex number, represented by two 32-bit ﬂoats (real and imaginary components) Complex number, represented by two 64-bit ﬂoats (real and imaginary components)

Additionally to intc the platform dependent C integer types short, long, longlong and their unsigned versions are deﬁned.

Numpy numerical types are instances of dtype (data-type) objects, each having unique characteristics. Once you have imported NumPy using

27

NumPy User Guide, Release 1.11.0
>>> import numpy as np
the dtypes are available as np.bool_, np.float32, etc. Advanced types, not listed in the table above, are explored in section Structured arrays. There are 5 basic numerical types representing booleans (bool), integers (int), unsigned integers (uint) ﬂoating point (ﬂoat) and complex. Those with numbers in their name indicate the bitsize of the type (i.e. how many bits are needed to represent a single value in memory). Some types, such as int and intp, have differing bitsizes, dependent on the platforms (e.g. 32-bit vs. 64-bit machines). This should be taken into account when interfacing with low-level code (such as C or Fortran) where the raw memory is addressed. Data-types can be used as functions to convert python numbers to array scalars (see the array scalar section for an explanation), python sequences of numbers to arrays of that type, or as arguments to the dtype keyword that many numpy functions or methods accept. Some examples:
>>> import numpy as np >>> x = np.float32(1.0) >>> x 1.0 >>> y = np.int_([1,2,4]) >>> y array([1, 2, 4]) >>> z = np.arange(3, dtype=np.uint8) >>> z array([0, 1, 2], dtype=uint8)
Array types can also be referred to by character codes, mostly to retain backward compatibility with older packages such as Numeric. Some documentation may still refer to these, for example:
>>> np.array([1, 2, 3], dtype='f') array([ 1., 2., 3.], dtype=float32)
We recommend using dtype objects instead. To convert the type of an array, use the .astype() method (preferred) or the type itself as a function. For example:
>>> z.astype(float) array([ 0., 1., 2.]) >>> np.int8(z) array([0, 1, 2], dtype=int8)
Note that, above, we use the Python ﬂoat object as a dtype. NumPy knows that int refers to np.int_, bool means np.bool_, that float is np.float_ and complex is np.complex_. The other data-types do not have Python equivalents. To determine the type of an array, look at the dtype attribute:
>>> z.dtype dtype('uint8')
dtype objects also contain information about the type, such as its bit-width and its byte-order. The data type can also be used indirectly to query properties of the type, such as whether it is an integer:
>>> d = np.dtype(int) >>> d dtype('int32')
>>> np.issubdtype(d, int) True
28 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0

>>> np.issubdtype(d, float) False

3.1.2 Array Scalars
Numpy generally returns elements of arrays as array scalars (a scalar with an associated dtype). Array scalars differ from Python scalars, but for the most part they can be used interchangeably (the primary exception is for versions of Python older than v2.x, where integer array scalars cannot act as indices for lists and tuples). There are some exceptions, such as when code requires very speciﬁc attributes of a scalar or when it checks speciﬁcally whether a value is a Python scalar. Generally, problems are easily ﬁxed by explicitly converting array scalars to Python scalars, using the corresponding Python type function (e.g., int, float, complex, str, unicode).
The primary advantage of using array scalars is that they preserve the array type (Python may not have a matching scalar type available, e.g. int16). Therefore, the use of array scalars ensures identical behaviour between arrays and scalars, irrespective of whether the value is inside an array or not. NumPy scalars also have many of the same methods arrays do.
3.1.3 Extended Precision
Python’s ﬂoating-point numbers are usually 64-bit ﬂoating-point numbers, nearly equivalent to np.float64. In some unusual situations it may be useful to use ﬂoating-point numbers with more precision. Whether this is possible in numpy depends on the hardware and on the development environment: speciﬁcally, x86 machines provide hardware ﬂoating-point with 80-bit precision, and while most C compilers provide this as their long double type, MSVC (standard for Windows builds) makes long double identical to double (64 bits). Numpy makes the compiler’s long double available as np.longdouble (and np.clongdouble for the complex numbers). You can ﬁnd out what your numpy provides with‘‘np.ﬁnfo(np.longdouble)‘‘.
Numpy does not provide a dtype with more precision than C long double‘‘s; in particular, the 128-bit IEEE quad precision data type (FORTRAN’s ‘‘REAL*16) is not available.
For efﬁcient memory alignment, np.longdouble is usually stored padded with zero bits, either to 96 or 128 bits. Which is more efﬁcient depends on hardware and development environment; typically on 32-bit systems they are padded to 96 bits, while on 64-bit systems they are typically padded to 128 bits. np.longdouble is padded to the system default; np.float96 and np.float128 are provided for users who want speciﬁc padding. In spite of the names, np.float96 and np.float128 provide only as much precision as np.longdouble, that is, 80 bits on most x86 machines and 64 bits in standard Windows builds.
Be warned that even if np.longdouble offers more precision than python float, it is easy to lose that extra precision, since python often forces values to pass through float. For example, the % formatting operator requires its arguments to be converted to standard python types, and it is therefore impossible to preserve extended precision even if many decimal places are requested. It can be useful to test your code with the value 1 + np.finfo(np.longdouble).eps.

3.2 Array creation
See also: Array creation routines
3.2.1 Introduction
There are 5 general mechanisms for creating arrays:
3.2. Array creation

29

NumPy User Guide, Release 1.11.0
1. Conversion from other Python structures (e.g., lists, tuples) 2. Intrinsic numpy array array creation objects (e.g., arange, ones, zeros, etc.) 3. Reading arrays from disk, either from standard or custom formats 4. Creating arrays from raw bytes through the use of strings or buffers 5. Use of special library functions (e.g., random) This section will not cover means of replicating, joining, or otherwise expanding or mutating existing arrays. Nor will it cover creating object arrays or structured arrays. Both of those are covered in their own sections.
3.2.2 Converting Python array_like Objects to Numpy Arrays
In general, numerical data arranged in an array-like structure in Python can be converted to arrays through the use of the array() function. The most obvious examples are lists and tuples. See the documentation for array() for details for its use. Some objects may support the array-protocol and allow conversion to arrays this way. A simple way to ﬁnd out if the object can be converted to a numpy array using array() is simply to try it interactively and see if it works! (The Python Way). Examples: >>> x = np.array([2,3,1,0]) >>> x = np.array([2, 3, 1, 0]) >>> x = np.array([[1,2.0],[0,0],(1+1j,3.)]) # note mix of tuple and lists,
and types >>> x = np.array([[ 1.+0.j, 2.+0.j], [ 0.+0.j, 0.+0.j], [ 1.+1.j, 3.+0.j]])
3.2.3 Intrinsic Numpy Array Creation
Numpy has built-in functions for creating arrays from scratch: zeros(shape) will create an array ﬁlled with 0 values with the speciﬁed shape. The default dtype is ﬂoat64. >>> np.zeros((2, 3)) array([[ 0., 0., 0.], [ 0., 0., 0.]]) ones(shape) will create an array ﬁlled with 1 values. It is identical to zeros in all other respects. arange() will create arrays with regularly incrementing values. Check the docstring for complete information on the various ways it can be used. A few examples will be given here: >>> np.arange(10) array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.arange(2, 10, dtype=np.float) array([ 2., 3., 4., 5., 6., 7., 8., 9.]) >>> np.arange(2, 3, 0.1) array([ 2. , 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9])
Note that there are some subtleties regarding the last usage that the user should be aware of that are described in the arange docstring. linspace() will create arrays with a speciﬁed number of elements, and spaced equally between the speciﬁed beginning and end values. For example: >>> np.linspace(1., 4., 6) array([ 1. , 1.6, 2.2, 2.8, 3.4, 4. ])
30 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0
The advantage of this creation function is that one can guarantee the number of elements and the starting and end point, which arange() generally will not do for arbitrary start, stop, and step values. indices() will create a set of arrays (stacked as a one-higher dimensioned array), one per dimension with each representing variation in that dimension. An example illustrates much better than a verbal description: >>> np.indices((3,3)) array([[[0, 0, 0], [1, 1, 1], [2, 2, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]]])
This is particularly useful for evaluating functions of multiple dimensions on a regular grid.
3.2.4 Reading Arrays From Disk
This is presumably the most common case of large array creation. The details, of course, depend greatly on the format of data on disk and so this section can only give general pointers on how to handle various formats.
Standard Binary Formats
Various ﬁelds have standard formats for array data. The following lists the ones with known python libraries to read them and return numpy arrays (there may be others for which it is possible to read and convert to numpy arrays so check the last section as well) HDF5: PyTables FITS: PyFITS
Examples of formats that cannot be read directly but for which it is not hard to convert are those formats supported by libraries like PIL (able to read and write many image formats such as jpg, png, etc).
Common ASCII Formats
Comma Separated Value ﬁles (CSV) are widely used (and an export and import option for programs like Excel). There are a number of ways of reading these ﬁles in Python. There are CSV functions in Python and functions in pylab (part of matplotlib). More generic ascii ﬁles can be read using the io package in scipy.
Custom Binary Formats
There are a variety of approaches one can use. If the ﬁle has a relatively simple format then one can write a simple I/O library and use the numpy fromﬁle() function and .toﬁle() method to read and write numpy arrays directly (mind your byteorder though!) If a good C or C++ library exists that read the data, one can wrap that library with a variety of techniques though that certainly is much more work and requires signiﬁcantly more advanced knowledge to interface with C or C++.
Use of Special Libraries
There are libraries that can be used to generate arrays for special purposes and it isn’t possible to enumerate all of them. The most common uses are use of the many array generation functions in random that can generate arrays of random values, and some utility functions to generate special matrices (e.g. diagonal).

3.2. Array creation

31

NumPy User Guide, Release 1.11.0
3.3 I/O with Numpy
3.3.1 Importing data with genfromtxt
Numpy provides several functions to create arrays from tabular data. We focus here on the genfromtxt function. In a nutshell, genfromtxt runs two main loops. The ﬁrst loop converts each line of the ﬁle in a sequence of strings. The second loop converts each string to the appropriate data type. This mechanism is slower than a single loop, but gives more ﬂexibility. In particular, genfromtxt is able to take missing data into account, when other faster and simpler functions like loadtxt cannot.
Note: When giving examples, we will use the following conventions: >>> import numpy as np >>> from StringIO import StringIO
Deﬁning the input
The only mandatory argument of genfromtxt is the source of the data. It can be a string, a list of strings, or a generator. If a single string is provided, it is assumed to be the name of a local or remote ﬁle, or a open ﬁle-like object with a read method, for example, a ﬁle or StringIO.StringIO object. If a list of strings or a generator returning strings is provided, each string is treated as one line in a ﬁle. When the URL of a remote ﬁle is passed, the ﬁle is automatically downloaded to the current directory and opened. Recognized ﬁle types are text ﬁles and archives. Currently, the function recognizes gzip and bz2 (bzip2) archives. The type of the archive is determined from the extension of the ﬁle: if the ﬁlename ends with ’.gz’, a gzip archive is expected; if it ends with ’bz2’, a bzip2 archive is assumed.
Splitting the lines into columns
The delimiter argument Once the ﬁle is deﬁned and open for reading, genfromtxt splits each non-empty line into a sequence of strings. Empty or commented lines are just skipped. The delimiter keyword is used to deﬁne how the splitting should take place. Quite often, a single character marks the separation between columns. For example, comma-separated ﬁles (CSV) use a comma (,) or a semicolon (;) as delimiter: >>> data = "1, 2, 3\n4, 5, 6" >>> np.genfromtxt(StringIO(data), delimiter=",") array([[ 1., 2., 3.],
[ 4., 5., 6.]])
Another common separator is "\t", the tabulation character. However, we are not limited to a single character, any string will do. By default, genfromtxt assumes delimiter=None, meaning that the line is split along white spaces (including tabs) and that consecutive white spaces are considered as a single white space. Alternatively, we may be dealing with a ﬁxed-width ﬁle, where columns are deﬁned as a given number of characters. In that case, we need to set delimiter to a single integer (if all the columns have the same size) or to a sequence of integers (if columns can have different sizes): >>> data = " 1 2 3\n 4 5 67\n890123 4" >>> np.genfromtxt(StringIO(data), delimiter=3) array([[ 1., 2., 3.],
32 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0

[ 4., 5., 67.],

[ 890., 123., 4.]])

>>> data = "123456789\n 4 7 9\n 4567 9"

>>> np.genfromtxt(StringIO(data), delimiter=(4, 3, 2))

array([[ 1234., 567., 89.],

[ 4., 7., 9.],

[ 4., 567.,

9.]])

The autostrip argument
By default, when a line is decomposed into a series of strings, the individual entries are not stripped of leading nor trailing white spaces. This behavior can be overwritten by setting the optional argument autostrip to a value of True:
>>> data = "1, abc , 2\n 3, xxx, 4" >>> # Without autostrip >>> np.genfromtxt(StringIO(data), delimiter=",", dtype="|S5") array([['1', ' abc ', ' 2'],
['3', ' xxx', ' 4']], dtype='|S5') >>> # With autostrip >>> np.genfromtxt(StringIO(data), delimiter=",", dtype="|S5", autostrip=True) array([['1', 'abc', '2'],
['3', 'xxx', '4']], dtype='|S5')

The comments argument
The optional argument comments is used to deﬁne a character string that marks the beginning of a comment. By default, genfromtxt assumes comments=’#’. The comment marker may occur anywhere on the line. Any character present after the comment marker(s) is simply ignored:
>>> data = """# ... # Skip me ! ... # Skip me too ! ... 1, 2 ... 3, 4 ... 5, 6 #This is the third line of the data ... 7, 8 ... # And here comes the last line ... 9, 0 ... """ >>> np.genfromtxt(StringIO(data), comments="#", delimiter=",") [[ 1. 2.]
[ 3. 4.] [ 5. 6.] [ 7. 8.] [ 9. 0.]]

Note: There is one notable exception to this behavior: if the optional argument names=True, the ﬁrst commented line will be examined for names.

3.3. I/O with Numpy

33

NumPy User Guide, Release 1.11.0

Skipping lines and choosing columns

The skip_header and skip_footer arguments

The presence of a header in the ﬁle can hinder data processing. In that case, we need to use the skip_header optional argument. The values of this argument must be an integer which corresponds to the number of lines to skip at the beginning of the ﬁle, before any other action is performed. Similarly, we can skip the last n lines of the ﬁle by using the skip_footer attribute and giving it a value of n:

>>> data = "\n".join(str(i) for i in range(10)) >>> np.genfromtxt(StringIO(data),) array([ 0., 1., 2., 3., 4., 5., 6., 7., 8., >>> np.genfromtxt(StringIO(data), ... skip_header=3, skip_footer=5) array([ 3., 4.])

9.])

By default, skip_header=0 and skip_footer=0, meaning that no lines are skipped.
The usecols argument
In some cases, we are not interested in all the columns of the data but only a few of them. We can select which columns to import with the usecols argument. This argument accepts a single integer or a sequence of integers corresponding to the indices of the columns to import. Remember that by convention, the ﬁrst column has an index of 0. Negative integers behave the same as regular Python negative indexes.
For example, if we want to import only the ﬁrst and the last columns, we can use usecols=(0, -1):
>>> data = "1 2 3\n4 5 6" >>> np.genfromtxt(StringIO(data), usecols=(0, -1)) array([[ 1., 3.],
[ 4., 6.]])

If the columns have names, we can also select which columns to import by giving their name to the usecols argument, either as a sequence of strings or a comma-separated string:
>>> data = "1 2 3\n4 5 6" >>> np.genfromtxt(StringIO(data), ... names="a, b, c", usecols=("a", "c")) array([(1.0, 3.0), (4.0, 6.0)],
dtype=[('a', '<f8'), ('c', '<f8')]) >>> np.genfromtxt(StringIO(data), ... names="a, b, c", usecols=("a, c"))
array([(1.0, 3.0), (4.0, 6.0)], dtype=[('a', '<f8'), ('c', '<f8')])

Choosing the data type
The main way to control how the sequences of strings we have read from the ﬁle are converted to other types is to set the dtype argument. Acceptable values for this argument are:
• a single type, such as dtype=float. The output will be 2D with the given dtype, unless a name has been associated with each column with the use of the names argument (see below). Note that dtype=float is the default for genfromtxt.
• a sequence of types, such as dtype=(int, float, float). • a comma-separated string, such as dtype="i4,f8,|S3". • a dictionary with two keys ’names’ and ’formats’.

34 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0
• a sequence of tuples (name, type), such as dtype=[(’A’, int), (’B’, float)].
• an existing numpy.dtype object.
• the special value None. In that case, the type of the columns will be determined from the data itself (see below).
In all the cases but the ﬁrst one, the output will be a 1D array with a structured dtype. This dtype has as many ﬁelds as items in the sequence. The ﬁeld names are deﬁned with the names keyword.
When dtype=None, the type of each column is determined iteratively from its data. We start by checking whether a string can be converted to a boolean (that is, if the string matches true or false in lower cases); then whether it can be converted to an integer, then to a ﬂoat, then to a complex and eventually to a string. This behavior may be changed by modifying the default mapper of the StringConverter class.
The option dtype=None is provided for convenience. However, it is signiﬁcantly slower than setting the dtype explicitly.
Setting the names
The names argument
A natural approach when dealing with tabular data is to allocate a name to each column. A ﬁrst possibility is to use an explicit structured dtype, as mentioned previously:
>>> data = StringIO("1 2 3\n 4 5 6") >>> np.genfromtxt(data, dtype=[(_, int) for _ in "abc"]) array([(1, 2, 3), (4, 5, 6)],
dtype=[('a', '<i8'), ('b', '<i8'), ('c', '<i8')])
Another simpler possibility is to use the names keyword with a sequence of strings or a comma-separated string:
>>> data = StringIO("1 2 3\n 4 5 6") >>> np.genfromtxt(data, names="A, B, C") array([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0)],
dtype=[('A', '<f8'), ('B', '<f8'), ('C', '<f8')])
In the example above, we used the fact that by default, dtype=float. By giving a sequence of names, we are forcing the output to a structured dtype.
We may sometimes need to deﬁne the column names from the data itself. In that case, we must use the names keyword with a value of True. The names will then be read from the ﬁrst line (after the skip_header ones), even if the line is commented out:
>>> data = StringIO("So it goes\n#a b c\n1 2 3\n 4 5 6") >>> np.genfromtxt(data, skip_header=1, names=True) array([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0)],
dtype=[('a', '<f8'), ('b', '<f8'), ('c', '<f8')])
The default value of names is None. If we give any other value to the keyword, the new names will overwrite the ﬁeld names we may have deﬁned with the dtype:
>>> data = StringIO("1 2 3\n 4 5 6") >>> ndtype=[('a',int), ('b', float), ('c', int)] >>> names = ["A", "B", "C"] >>> np.genfromtxt(data, names=names, dtype=ndtype) array([(1, 2.0, 3), (4, 5.0, 6)],
dtype=[('A', '<i8'), ('B', '<f8'), ('C', '<i8')])

3.3. I/O with Numpy

35

NumPy User Guide, Release 1.11.0
The defaultfmt argument If names=None but a structured dtype is expected, names are deﬁned with the standard NumPy default of "f%i", yielding names like f0, f1 and so forth:
>>> data = StringIO("1 2 3\n 4 5 6") >>> np.genfromtxt(data, dtype=(int, float, int)) array([(1, 2.0, 3), (4, 5.0, 6)],
dtype=[('f0', '<i8'), ('f1', '<f8'), ('f2', '<i8')])
In the same way, if we don’t give enough names to match the length of the dtype, the missing names will be deﬁned with this default template:
>>> data = StringIO("1 2 3\n 4 5 6") >>> np.genfromtxt(data, dtype=(int, float, int), names="a") array([(1, 2.0, 3), (4, 5.0, 6)],
dtype=[('a', '<i8'), ('f0', '<f8'), ('f1', '<i8')])
We can overwrite this default with the defaultfmt argument, that takes any format string:
>>> data = StringIO("1 2 3\n 4 5 6") >>> np.genfromtxt(data, dtype=(int, float, int), defaultfmt="var_%02i") array([(1, 2.0, 3), (4, 5.0, 6)],
dtype=[('var_00', '<i8'), ('var_01', '<f8'), ('var_02', '<i8')])
Note: We need to keep in mind that defaultfmt is used only if some names are expected but not deﬁned.
Validating names Numpy arrays with a structured dtype can also be viewed as recarray, where a ﬁeld can be accessed as if it were an attribute. For that reason, we may need to make sure that the ﬁeld name doesn’t contain any space or invalid character, or that it does not correspond to the name of a standard attribute (like size or shape), which would confuse the interpreter. genfromtxt accepts three optional arguments that provide a ﬁner control on the names:
deletechars Gives a string combining all the characters that must be deleted from the name. By default, invalid characters are ~!@#$%^&*()-=+~\|]}[{’;: /?.>,<.
excludelist Gives a list of the names to exclude, such as return, file, print... If one of the input name is part of this list, an underscore character (’_’) will be appended to it.
case_sensitive Whether the names should be case-sensitive (case_sensitive=True), converted to upper case (case_sensitive=False or case_sensitive=’upper’) or to lower case (case_sensitive=’lower’).
Tweaking the conversion
The converters argument Usually, deﬁning a dtype is sufﬁcient to deﬁne how the sequence of strings must be converted. However, some additional control may sometimes be required. For example, we may want to make sure that a date in a format YYYY/MM/DD is converted to a datetime object, or that a string like xx% is properly converted to a ﬂoat between 0 and 1. In such cases, we should deﬁne conversion functions with the converters arguments. The value of this argument is typically a dictionary with column indices or column names as keys and a conversion functions as values. These conversion functions can either be actual functions or lambda functions. In any case, they should accept only a string as input and output only a single element of the wanted type.
36 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0

In the following example, the second column is converted from as string representing a percentage to a ﬂoat between 0 and 1:
>>> convertfunc = lambda x: float(x.strip("%"))/100. >>> data = "1, 2.3%, 45.\n6, 78.9%, 0" >>> names = ("i", "p", "n") >>> # General case ..... >>> np.genfromtxt(StringIO(data), delimiter=",", names=names) array([(1.0, nan, 45.0), (6.0, nan, 0.0)],
dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])
We need to keep in mind that by default, dtype=float. A ﬂoat is therefore expected for the second column. However, the strings ’ 2.3%’ and ’ 78.9%’ cannot be converted to ﬂoat and we end up having np.nan instead. Let’s now use a converter:
>>> # Converted case ... >>> np.genfromtxt(StringIO(data), delimiter=",", names=names, ... converters={1: convertfunc}) array([(1.0, 0.023, 45.0), (6.0, 0.78900000000000003, 0.0)],
dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])
The same results can be obtained by using the name of the second column ("p") as key instead of its index (1):
>>> # Using a name for the converter ... >>> np.genfromtxt(StringIO(data), delimiter=",", names=names, ... converters={"p": convertfunc}) array([(1.0, 0.023, 45.0), (6.0, 0.78900000000000003, 0.0)],
dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])
Converters can also be used to provide a default for missing entries. In the following example, the converter convert transforms a stripped string into the corresponding ﬂoat or into -999 if the string is empty. We need to explicitly strip the string from white spaces as it is not done by default:
>>> data = "1, , 3\n 4, 5, 6" >>> convert = lambda x: float(x.strip() or -999) >>> np.genfromtxt(StringIO(data), delimiter=",", ... converter={1: convert}) array([[ 1., -999., 3.],
[ 4., 5., 6.]])
Using missing and ﬁlling values
Some entries may be missing in the dataset we are trying to import. In a previous example, we used a converter to transform an empty string into a ﬂoat. However, user-deﬁned converters may rapidly become cumbersome to manage.
The genfromtxt function provides two other complementary mechanisms: the missing_values argument is used to recognize missing data and a second argument, filling_values, is used to process these missing data.
missing_values
By default, any empty string is marked as missing. We can also consider more complex strings, such as "N/A" or "???" to represent missing or invalid data. The missing_values argument accepts three kind of values:
a string or a comma-separated string This string will be used as the marker for missing data for all the columns
a sequence of strings In that case, each item is associated to a column, in order.
a dictionary Values of the dictionary are strings or sequence of strings. The corresponding keys can be column

3.3. I/O with Numpy

37

NumPy User Guide, Release 1.11.0

indices (integers) or column names (strings). In addition, the special key None can be used to deﬁne a default applicable to all columns.

filling_values

We know how to recognize missing data, but we still need to provide a value for these missing entries. By default, this value is determined from the expected dtype according to this table:

Expected type bool int float complex string

Default False -1 np.nan np.nan+0j ’???’

We can get a ﬁner control on the conversion of missing values with the filling_values optional argument. Like missing_values, this argument accepts different kind of values:

a single value This will be the default for all columns

a sequence of values Each entry will be the default for the corresponding column

a dictionary Each key can be a column index or a column name, and the corresponding value should be a single object. We can use the special key None to deﬁne a default for all columns.

In the following example, we suppose that the missing values are ﬂagged with "N/A" in the ﬁrst column and by "???" in the third column. We wish to transform these missing values to 0 if they occur in the ﬁrst and second column, and to -999 if they occur in the last column:

>>> data = "N/A, 2, 3\n4, ,???" >>> kwargs = dict(delimiter=",", ... dtype=int, ... names="a,b,c", ... missing_values={0:"N/A", 'b':" ", 2:"???"}, ... filling_values={0:0, 'b':0, 2:-999}) >>> np.genfromtxt(StringIO.StringIO(data), **kwargs) array([(0, 2, 3), (4, 0, -999)],
dtype=[('a', '<i8'), ('b', '<i8'), ('c', '<i8')])

usemask
We may also want to keep track of the occurrence of missing data by constructing a boolean mask, with True entries where data was missing and False otherwise. To do that, we just have to set the optional argument usemask to True (the default is False). The output array will then be a MaskedArray.

Shortcut functions
In addition to genfromtxt, the numpy.lib.io module provides several convenience functions derived from genfromtxt. These functions work the same way as the original, but they have different default values.
ndfromtxt Always set usemask=False. The output is always a standard numpy.ndarray.
mafromtxt Always set usemask=True. The output is always a MaskedArray

38 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0
recfromtxt Returns a standard numpy.recarray (if usemask=False) or a MaskedRecords array (if usemaske=True). The default dtype is dtype=None, meaning that the types of each column will be automatically determined.
recfromcsv Like recfromtxt, but with a default delimiter=",".
3.4 Indexing
See also: Indexing routines Array indexing refers to any use of the square brackets ([]) to index array values. There are many options to indexing, which give numpy indexing great power, but with power comes some complexity and the potential for confusion. This section is just an overview of the various options and issues related to indexing. Aside from single element indexing, the details on most of these options are to be found in related sections.
3.4.1 Assignment vs referencing
Most of the following examples show the use of indexing when referencing data in an array. The examples work just as well when assigning to an array. See the section at the end for speciﬁc examples and explanations on how assignments work.
3.4.2 Single element indexing
Single element indexing for a 1-D array is what one expects. It work exactly like that for other standard Python sequences. It is 0-based, and accepts negative indices for indexing from the end of the array.
>>> x = np.arange(10) >>> x[2] 2 >>> x[-2] 8
Unlike lists and tuples, numpy arrays support multidimensional indexing for multidimensional arrays. That means that it is not necessary to separate each dimension’s index into its own set of square brackets.
>>> x.shape = (2,5) # now x is 2-dimensional >>> x[1,3] 8 >>> x[1,-1] 9
Note that if one indexes a multidimensional array with fewer indices than dimensions, one gets a subdimensional array. For example:
>>> x[0] array([0, 1, 2, 3, 4])
That is, each index speciﬁed selects the array corresponding to the rest of the dimensions selected. In the above example, choosing 0 means that the remaining dimension of length 5 is being left unspeciﬁed, and that what is returned is an array of that dimensionality and size. It must be noted that the returned array is not a copy of the original, but

3.4. Indexing

39

NumPy User Guide, Release 1.11.0
points to the same values in memory as does the original array. In this case, the 1-D array at the ﬁrst position (0) is returned. So using a single index on the returned array, results in a single element being returned. That is:
>>> x[0][2] 2
So note that x[0,2] = x[0][2] though the second case is more inefﬁcient as a new temporary array is created after the ﬁrst index that is subsequently indexed by 2.
Note to those used to IDL or Fortran memory order as it relates to indexing. Numpy uses C-order indexing. That means that the last index usually represents the most rapidly changing memory location, unlike Fortran or IDL, where the ﬁrst index represents the most rapidly changing location in memory. This difference represents a great potential for confusion.
3.4.3 Other indexing options
It is possible to slice and stride arrays to extract arrays of the same number of dimensions, but of different sizes than the original. The slicing and striding works exactly the same way it does for lists and tuples except that they can be applied to multiple dimensions as well. A few examples illustrates best:
>>> x = np.arange(10) >>> x[2:5] array([2, 3, 4]) >>> x[:-7] array([0, 1, 2]) >>> x[1:7:2] array([1, 3, 5]) >>> y = np.arange(35).reshape(5,7) >>> y[1:5:2,::3] array([[ 7, 10, 13],
[21, 24, 27]])
Note that slices of arrays do not copy the internal array data but also produce new views of the original data.
It is possible to index arrays with other arrays for the purposes of selecting lists of values out of arrays into new arrays. There are two different ways of accomplishing this. One uses one or more arrays of index values. The other involves giving a boolean array of the proper shape to indicate the values to be selected. Index arrays are a very powerful tool that allow one to avoid looping over individual elements in arrays and thus greatly improve performance.
It is possible to use special features to effectively increase the number of dimensions in an array through indexing so the resulting array aquires the shape needed for use in an expression or with a speciﬁc function.
3.4.4 Index arrays
Numpy arrays may be indexed with other arrays (or any other sequence- like object that can be converted to an array, such as lists, with the exception of tuples; see the end of this document for why this is). The use of index arrays ranges from simple, straightforward cases to complex, hard-to-understand cases. For all cases of index arrays, what is returned is a copy of the original data, not a view as one gets for slices.
Index arrays must be of integer type. Each value in the array indicates which value in the array to use in place of the index. To illustrate:
>>> x = np.arange(10,1,-1) >>> x array([10, 9, 8, 7, 6, 5, 4, 3, 2]) >>> x[np.array([3, 3, 1, 8])] array([7, 7, 9, 2])
40 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0
The index array consisting of the values 3, 3, 1 and 8 correspondingly create an array of length 4 (same as the index array) where each index is replaced by the value the index array has in the array being indexed. Negative values are permitted and work as they do with single indices or slices:
>>> x[np.array([3,3,-3,8])] array([7, 7, 4, 2])
It is an error to have index values out of bounds:
>>> x[np.array([3, 3, 20, 8])] <type 'exceptions.IndexError'>: index 20 out of bounds 0<=index<9
Generally speaking, what is returned when index arrays are used is an array with the same shape as the index array, but with the type and values of the array being indexed. As an example, we can use a multidimensional index array instead:
>>> x[np.array([[1,1],[2,3]])] array([[9, 9],
[8, 7]])
3.4.5 Indexing Multi-dimensional arrays
Things become more complex when multidimensional arrays are indexed, particularly with multidimensional index arrays. These tend to be more unusal uses, but theyare permitted, and they are useful for some problems. We’ll start with thesimplest multidimensional case (using the array y from the previous examples):
>>> y[np.array([0,2,4]), np.array([0,1,2])] array([ 0, 15, 30])
In this case, if the index arrays have a matching shape, and there is an index array for each dimension of the array being indexed, the resultant array has the same shape as the index arrays, and the values correspond to the index set for each position in the index arrays. In this example, the ﬁrst index value is 0 for both index arrays, and thus the ﬁrst value of the resultant array is y[0,0]. The next value is y[2,1], and the last is y[4,2]. If the index arrays do not have the same shape, there is an attempt to broadcast them to the same shape. If they cannot be broadcast to the same shape, an exception is raised:
>>> y[np.array([0,2,4]), np.array([0,1])] <type 'exceptions.ValueError'>: shape mismatch: objects cannot be broadcast to a single shape
The broadcasting mechanism permits index arrays to be combined with scalars for other indices. The effect is that the scalar value is used for all the corresponding values of the index arrays:
>>> y[np.array([0,2,4]), 1] array([ 1, 15, 29])
Jumping to the next level of complexity, it is possible to only partially index an array with index arrays. It takes a bit of thought to understand what happens in such cases. For example if we just use one index array with y:
>>> y[np.array([0,2,4])] array([[ 0, 1, 2, 3, 4, 5, 6],
[14, 15, 16, 17, 18, 19, 20], [28, 29, 30, 31, 32, 33, 34]])
What results is the construction of a new array where each value of the index array selects one row from the array being indexed and the resultant array has the resulting shape (size of row, number index elements).

3.4. Indexing

41

NumPy User Guide, Release 1.11.0
An example of where this may be useful is for a color lookup table where we want to map the values of an image into RGB triples for display. The lookup table could have a shape (nlookup, 3). Indexing such an array with an image with shape (ny, nx) with dtype=np.uint8 (or any integer type so long as values are with the bounds of the lookup table) will result in an array of shape (ny, nx, 3) where a triple of RGB values is associated with each pixel location.
In general, the shape of the resulant array will be the concatenation of the shape of the index array (or the shape that all the index arrays were broadcast to) with the shape of any unused dimensions (those not indexed) in the array being indexed.
3.4.6 Boolean or “mask” index arrays
Boolean arrays used as indices are treated in a different manner entirely than index arrays. Boolean arrays must be of the same shape as the initial dimensions of the array being indexed. In the most straightforward case, the boolean array has the same shape:
>>> b = y>20 >>> y[b] array([21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34])
Unlike in the case of integer index arrays, in the boolean case, the result is a 1-D array containing all the elements in the indexed array corresponding to all the true elements in the boolean array. The elements in the indexed array are always iterated and returned in row-major (C-style) order. The result is also identical to y[np.nonzero(b)]. As with index arrays, what is returned is a copy of the data, not a view as one gets with slices.
The result will be multidimensional if y has more dimensions than b. For example:
>>> b[:,5] # use a 1-D boolean whose first dim agrees with the first dim of y array([False, False, False, True, True], dtype=bool) >>> y[b[:,5]] array([[21, 22, 23, 24, 25, 26, 27],
[28, 29, 30, 31, 32, 33, 34]])
Here the 4th and 5th rows are selected from the indexed array and combined to make a 2-D array.
In general, when the boolean array has fewer dimensions than the array being indexed, this is equivalent to y[b, ...], which means y is indexed by b followed by as many : as are needed to ﬁll out the rank of y. Thus the shape of the result is one dimension containing the number of True elements of the boolean array, followed by the remaining dimensions of the array being indexed.
For example, using a 2-D boolean array of shape (2,3) with four True elements to select rows from a 3-D array of shape (2,3,5) results in a 2-D result of shape (4,5):
>>> x = np.arange(30).reshape(2,3,5) >>> x array([[[ 0, 1, 2, 3, 4],
[ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]], [[15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29]]]) >>> b = np.array([[True, True, False], [False, True, True]]) >>> x[b] array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29]])
For further details, consult the numpy reference documentation on array indexing.
42 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0

3.4.7 Combining index arrays with slices
Index arrays may be combined with slices. For example:
>>> y[np.array([0,2,4]),1:3] array([[ 1, 2],
[15, 16], [29, 30]])
In effect, the slice is converted to an index array np.array([[1,2]]) (shape (1,2)) that is broadcast with the index array to produce a resultant array of shape (3,2).
Likewise, slicing can be combined with broadcasted boolean indices:
>>> y[b[:,5],1:3] array([[22, 23],
[29, 30]])

3.4.8 Structural indexing tools
To facilitate easy matching of array shapes with expressions and in assignments, the np.newaxis object can be used within array indices to add new dimensions with a size of 1. For example:
>>> y.shape (5, 7) >>> y[:,np.newaxis,:].shape (5, 1, 7)
Note that there are no new elements in the array, just that the dimensionality is increased. This can be handy to combine two arrays in a way that otherwise would require explicitly reshaping operations. For example:
>>> x = np.arange(5) >>> x[:,np.newaxis] + x[np.newaxis,:] array([[0, 1, 2, 3, 4],
[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7], [4, 5, 6, 7, 8]])
The ellipsis syntax maybe used to indicate selecting in full any remaining unspeciﬁed dimensions. For example:
>>> z = np.arange(81).reshape(3,3,3,3) >>> z[1,...,2] array([[29, 32, 35],
[38, 41, 44], [47, 50, 53]])
This is equivalent to:
>>> z[1,:,:,2] array([[29, 32, 35],
[38, 41, 44], [47, 50, 53]])

3.4.9 Assigning values to indexed arrays
As mentioned, one can select a subset of an array to assign to using a single index, slices, and index and mask arrays. The value being assigned to the indexed array must be shape consistent (the same shape or broadcastable to the shape

3.4. Indexing

43

NumPy User Guide, Release 1.11.0
the index produces). For example, it is permitted to assign a constant to a slice:
>>> x = np.arange(10) >>> x[2:7] = 1
or an array of the right size:
>>> x[2:7] = np.arange(5)
Note that assignments may result in changes if assigning higher types to lower types (like ﬂoats to ints) or even exceptions (assigning complex to ﬂoats or ints):
>>> x[1] = 1.2 >>> x[1] 1 >>> x[1] = 1.2j <type 'exceptions.TypeError'>: can't convert complex to long; use long(abs(z))
Unlike some of the references (such as array and mask indices) assignments are always made to the original data in the array (indeed, nothing else would make sense!). Note though, that some actions may not work as one may naively expect. This particular example is often surprising to people:
>>> x = np.arange(0, 50, 10) >>> x array([ 0, 10, 20, 30, 40]) >>> x[np.array([1, 1, 3, 1])] += 1 >>> x array([ 0, 11, 20, 31, 40])
Where people expect that the 1st location will be incremented by 3. In fact, it will only be incremented by 1. The reason is because a new array is extracted from the original (as a temporary) containing the values at 1, 1, 3, 1, then the value 1 is added to the temporary, and then the temporary is assigned back to the original array. Thus the value of the array at x[1]+1 is assigned to x[1] three times, rather than being incremented 3 times.
3.4.10 Dealing with variable numbers of indices within programs
The index syntax is very powerful but limiting when dealing with a variable number of indices. For example, if you want to write a function that can handle arguments with various numbers of dimensions without having to write special case code for each number of possible dimensions, how can that be done? If one supplies to the index a tuple, the tuple will be interpreted as a list of indices. For example (using the previous deﬁnition for the array z):
>>> indices = (1,1,1,1) >>> z[indices] 40
So one can use code to construct tuples of any number of indices and then use these within an index. Slices can be speciﬁed within programs by using the slice() function in Python. For example:
>>> indices = (1,1,1,slice(0,2)) # same as [1,1,1,0:2] >>> z[indices] array([39, 40])
Likewise, ellipsis can be speciﬁed by code by using the Ellipsis object:
>>> indices = (1, Ellipsis, 1) # same as [1,...,1] >>> z[indices] array([[28, 31, 34],
44 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0

[37, 40, 43], [46, 49, 52]])
For this reason it is possible to use the output from the np.where() function directly as an index since it always returns a tuple of index arrays.
Because the special treatment of tuples, they are not automatically converted to an array as a list would be. As an example:
>>> z[[1,1,1,1]] # produces a large array array([[[[27, 28, 29],
[30, 31, 32], ... >>> z[(1,1,1,1)] # returns a single value 40

3.5 Broadcasting
See also:
numpy.broadcast
The term broadcasting describes how numpy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efﬁcient algorithm implementations. There are, however, cases where broadcasting is a bad idea because it leads to inefﬁcient use of memory that slows computation.
NumPy operations are usually done on pairs of arrays on an element-by-element basis. In the simplest case, the two arrays must have exactly the same shape, as in the following example:
>>> a = np.array([1.0, 2.0, 3.0]) >>> b = np.array([2.0, 2.0, 2.0]) >>> a * b array([ 2., 4., 6.])
NumPy’s broadcasting rule relaxes this constraint when the arrays’ shapes meet certain constraints. The simplest broadcasting example occurs when an array and a scalar value are combined in an operation:
>>> a = np.array([1.0, 2.0, 3.0]) >>> b = 2.0 >>> a * b array([ 2., 4., 6.])
The result is equivalent to the previous example where b was an array. We can think of the scalar b being stretched during the arithmetic operation into an array with the same shape as a. The new elements in b are simply copies of the original scalar. The stretching analogy is only conceptual. NumPy is smart enough to use the original scalar value without actually making copies, so that broadcasting operations are as memory and computationally efﬁcient as possible.
The code in the second example is more efﬁcient than that in the ﬁrst because broadcasting moves less memory around during the multiplication (b is a scalar rather than an array).
3.5.1 General Broadcasting Rules
When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when

3.5. Broadcasting

45

NumPy User Guide, Release 1.11.0

1. they are equal, or

2. one of them is 1

If these conditions are not met, a ValueError: frames are not aligned exception is thrown, indicating that the arrays have incompatible shapes. The size of the resulting array is the maximum size along each dimension of the input arrays.

Arrays do not need to have the same number of dimensions. For example, if you have a 256x256x3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a onedimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:

Image (3d array): 256 x 256 x 3

Scale (1d array):

3

Result (3d array): 256 x 256 x 3

When either of the dimensions compared is one, the other is used. In other words, dimensions with size 1 are stretched or “copied” to match the other.
In the following example, both the A and B arrays have axes with length one that are expanded to a larger size during the broadcast operation:
A (4d array): 8 x 1 x 6 x 1 B (3d array): 7 x 1 x 5 Result (4d array): 8 x 7 x 6 x 5

Here are some more examples:
A (2d array): 5 x 4 B (1d array): 1 Result (2d array): 5 x 4

A (2d array): 5 x 4 B (1d array): 4 Result (2d array): 5 x 4

A (3d array): 15 x 3 x 5 B (3d array): 15 x 1 x 5 Result (3d array): 15 x 3 x 5

A (3d array): 15 x 3 x 5

B (2d array):

3x5

Result (3d array): 15 x 3 x 5

A (3d array): 15 x 3 x 5

B (2d array):

3x1

Result (3d array): 15 x 3 x 5

Here are examples of shapes that do not broadcast:
A (1d array): 3 B (1d array): 4 # trailing dimensions do not match

A (2d array): 2 x 1 B (3d array): 8 x 4 x 3 # second from last dimensions mismatched

An example of broadcasting in practice:
>>> x = np.arange(4) >>> xx = x.reshape(4,1)

46 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0
>>> y = np.ones(5) >>> z = np.ones((3,4))
>>> x.shape (4,)
>>> y.shape (5,)
>>> x + y <type 'exceptions.ValueError'>: shape mismatch: objects cannot be broadcast to a single shape
>>> xx.shape (4, 1)
>>> y.shape (5,)
>>> (xx + y).shape (4, 5)
>>> xx + y array([[ 1., 1., 1., 1., 1.],
[ 2., 2., 2., 2., 2.], [ 3., 3., 3., 3., 3.], [ 4., 4., 4., 4., 4.]])
>>> x.shape (4,)
>>> z.shape (3, 4)
>>> (x + z).shape (3, 4)
>>> x + z array([[ 1., 2., 3., 4.],
[ 1., 2., 3., 4.], [ 1., 2., 3., 4.]])
Broadcasting provides a convenient way of taking the outer product (or any other outer operation) of two arrays. The following example shows an outer addition operation of two 1-d arrays:
>>> a = np.array([0.0, 10.0, 20.0, 30.0]) >>> b = np.array([1.0, 2.0, 3.0]) >>> a[:, np.newaxis] + b array([[ 1., 2., 3.],
[ 11., 12., 13.], [ 21., 22., 23.], [ 31., 32., 33.]])
Here the newaxis index operator inserts a new axis into a, making it a two-dimensional 4x1 array. Combining the 4x1 array with b, which has shape (3,), yields a 4x3 array. See this article for illustrations of broadcasting concepts.

3.5. Broadcasting

47

NumPy User Guide, Release 1.11.0
3.6 Byte-swapping
3.6.1 Introduction to byte ordering and ndarrays
The ndarray is an object that provide a python array interface to data in memory. It often happens that the memory that you want to view with an array is not of the same byte ordering as the computer on which you are running Python. For example, I might be working on a computer with a little-endian CPU - such as an Intel Pentium, but I have loaded some data from a ﬁle written by a computer that is big-endian. Let’s say I have loaded 4 bytes from a ﬁle written by a Sun (big-endian) computer. I know that these 4 bytes represent two 16-bit integers. On a big-endian machine, a two-byte integer is stored with the Most Signiﬁcant Byte (MSB) ﬁrst, and then the Least Signiﬁcant Byte (LSB). Thus the bytes are, in memory order:
1. MSB integer 1 2. LSB integer 1 3. MSB integer 2 4. LSB integer 2 Let’s say the two integers were in fact 1 and 770. Because 770 = 256 * 3 + 2, the 4 bytes in memory would contain respectively: 0, 1, 3, 2. The bytes I have loaded from the ﬁle would have these contents: >>> big_end_str = chr(0) + chr(1) + chr(3) + chr(2) >>> big_end_str '\x00\x01\x03\x02'
We might want to use an ndarray to access these integers. In that case, we can create an array around this memory, and tell numpy that there are two integers, and that they are 16 bit and big-endian:
>>> import numpy as np >>> big_end_arr = np.ndarray(shape=(2,),dtype='>i2', buffer=big_end_str) >>> big_end_arr[0] 1 >>> big_end_arr[1] 770
Note the array dtype above of >i2. The > means ‘big-endian’ (< is little-endian) and i2 means ‘signed 2-byte integer’. For example, if our data represented a single unsigned 4-byte little-endian integer, the dtype string would be <u4. In fact, why don’t we try that? >>> little_end_u4 = np.ndarray(shape=(1,),dtype='<u4', buffer=big_end_str) >>> little_end_u4[0] == 1 * 256**1 + 3 * 256**2 + 2 * 256**3 True
Returning to our big_end_arr - in this case our underlying data is big-endian (data endianness) and we’ve set the dtype to match (the dtype is also big-endian). However, sometimes you need to ﬂip these around.
Warning: Scalars currently do not include byte order information, so extracting a scalar from an array will return an integer in native byte order. Hence:
>>> big_end_arr[0].dtype.byteorder == little_end_u4[0].dtype.byteorder True
48 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0
3.6.2 Changing byte ordering
As you can imagine from the introduction, there are two ways you can affect the relationship between the byte ordering of the array and the underlying memory it is looking at:
• Change the byte-ordering information in the array dtype so that it interprets the undelying data as being in a different byte order. This is the role of arr.newbyteorder()
• Change the byte-ordering of the underlying data, leaving the dtype interpretation as it was. This is what arr.byteswap() does.
The common situations in which you need to change byte ordering are: 1. Your data and dtype endianess don’t match, and you want to change the dtype so that it matches the data. 2. Your data and dtype endianess don’t match, and you want to swap the data so that they match the dtype 3. Your data and dtype endianess match, but you want the data swapped and the dtype to reﬂect this
Data and dtype endianness don’t match, change dtype to match data
We make something where they don’t match: >>> wrong_end_dtype_arr = np.ndarray(shape=(2,),dtype='<i2', buffer=big_end_str) >>> wrong_end_dtype_arr[0] 256
The obvious ﬁx for this situation is to change the dtype so it gives the correct endianness: >>> fixed_end_dtype_arr = wrong_end_dtype_arr.newbyteorder() >>> fixed_end_dtype_arr[0] 1
Note the the array has not changed in memory: >>> fixed_end_dtype_arr.tobytes() == big_end_str True
Data and type endianness don’t match, change data to match dtype
You might want to do this if you need the data in memory to be a certain ordering. For example you might be writing the memory out to a ﬁle that needs a certain byte ordering. >>> fixed_end_mem_arr = wrong_end_dtype_arr.byteswap() >>> fixed_end_mem_arr[0] 1
Now the array has changed in memory: >>> fixed_end_mem_arr.tobytes() == big_end_str False
Data and dtype endianness match, swap data and dtype
You may have a correctly speciﬁed array dtype, but you need the array to have the opposite byte order in memory, and you want the dtype to match so the array values make sense. In this case you just do both of the previous operations:

3.6. Byte-swapping

49

NumPy User Guide, Release 1.11.0
>>> swapped_end_arr = big_end_arr.byteswap().newbyteorder() >>> swapped_end_arr[0] 1 >>> swapped_end_arr.tobytes() == big_end_str False
An easier way of casting the data to a speciﬁc dtype and byte ordering can be achieved with the ndarray astype method:
>>> swapped_end_arr = big_end_arr.astype('<i2') >>> swapped_end_arr[0] 1 >>> swapped_end_arr.tobytes() == big_end_str False
3.7 Structured arrays
3.7.1 Introduction
Numpy provides powerful capabilities to create arrays of structured datatype. These arrays permit one to manipulate the data by named ﬁelds. A simple example will show what is meant.:
>>> x = np.array([(1,2.,'Hello'), (2,3.,"World")], ... dtype=[('foo', 'i4'),('bar', 'f4'), ('baz', 'S10')]) >>> x array([(1, 2.0, 'Hello'), (2, 3.0, 'World')],
dtype=[('foo', '>i4'), ('bar', '>f4'), ('baz', '|S10')])
Here we have created a one-dimensional array of length 2. Each element of this array is a structure that contains three items, a 32-bit integer, a 32-bit ﬂoat, and a string of length 10 or less. If we index this array at the second position we get the second structure:
>>> x[1] (2,3.,"World")
Conveniently, one can access any ﬁeld of the array by indexing using the string that names that ﬁeld.
>>> y = x['bar'] >>> y array([ 2., 3.], dtype=float32) >>> y[:] = 2*y >>> y array([ 4., 6.], dtype=float32) >>> x array([(1, 4.0, 'Hello'), (2, 6.0, 'World')],
dtype=[('foo', '>i4'), ('bar', '>f4'), ('baz', '|S10')])
In these examples, y is a simple ﬂoat array consisting of the 2nd ﬁeld in the structured type. But, rather than being a copy of the data in the structured array, it is a view, i.e., it shares exactly the same memory locations. Thus, when we updated this array by doubling its values, the structured array shows the corresponding values as doubled as well. Likewise, if one changes the structured array, the ﬁeld view also changes:
>>> x[1] = (-1,-1.,"Master") >>> x array([(1, 4.0, 'Hello'), (-1, -1.0, 'Master')],
dtype=[('foo', '>i4'), ('bar', '>f4'), ('baz', '|S10')])
50 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0
>>> y array([ 4., -1.], dtype=float32)
3.7.2 Deﬁning Structured Arrays
One deﬁnes a structured array through the dtype object. There are several alternative ways to deﬁne the ﬁelds of a record. Some of these variants provide backward compatibility with Numeric, numarray, or another module, and should not be used except for such purposes. These will be so noted. One speciﬁes record structure in one of four alternative ways, using an argument (as supplied to a dtype function keyword or a dtype object constructor itself). This argument must be one of the following: 1) string, 2) tuple, 3) list, or 4) dictionary. Each of these is brieﬂy described below.
1) String argument. In this case, the constructor expects a comma-separated list of type speciﬁers, optionally with extra shape information. The ﬁelds are given the default names ‘f0’, ‘f1’, ‘f2’ and so on. The type speciﬁers can take 4 different forms:
a) b1, i1, i2, i4, i8, u1, u2, u4, u8, f2, f4, f8, c8, c16, a<n> (representing bytes, ints, unsigned ints, floats, complex and fixed length strings of specified byte lengths)
b) int8,...,uint8,...,float16, float32, float64, complex64, complex128 (this time with bit sizes)
c) older Numeric/numarray type specifications (e.g. Float32). Don't use these in new code!
d) Single character type specifiers (e.g H for unsigned short ints). Avoid using these unless you must. Details can be found in the Numpy book
These different styles can be mixed within the same string (but why would you want to do that?). Furthermore, each type speciﬁer can be preﬁxed with a repetition number, or a shape. In these cases an array element is created, i.e., an array within a record. That array is still referred to as a single ﬁeld. An example:
>>> x = np.zeros(3, dtype='3int8, float32, (2,3)float64') >>> x array([([0, 0, 0], 0.0, [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]),
([0, 0, 0], 0.0, [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]), ([0, 0, 0], 0.0, [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])], dtype=[('f0', '|i1', 3), ('f1', '>f4'), ('f2', '>f8', (2, 3))])
By using strings to deﬁne the record structure, it precludes being able to name the ﬁelds in the original deﬁnition. The names can be changed as shown later, however.
2) Tuple argument: The only relevant tuple case that applies to record structures is when a structure is mapped to an existing data type. This is done by pairing in a tuple, the existing data type with a matching dtype deﬁnition (using any of the variants being described here). As an example (using a deﬁnition using a list, so see 3) for further details):
>>> x = np.zeros(3, dtype=('i4',[('r','u1'), ('g','u1'), ('b','u1'), ('a','u1')])) >>> x array([0, 0, 0]) >>> x['r'] array([0, 0, 0], dtype=uint8)
In this case, an array is produced that looks and acts like a simple int32 array, but also has deﬁnitions for ﬁelds that use only one byte of the int32 (a bit like Fortran equivalencing).
3) List argument: In this case the record structure is deﬁned with a list of tuples. Each tuple has 2 or 3 elements specifying: 1) The name of the ﬁeld (‘’ is permitted), 2) the type of the ﬁeld, and 3) the shape (optional). For example:

3.7. Structured arrays

51

NumPy User Guide, Release 1.11.0
>>> x = np.zeros(3, dtype=[('x','f4'),('y',np.float32),('value','f4',(2,2))]) >>> x array([(0.0, 0.0, [[0.0, 0.0], [0.0, 0.0]]),
(0.0, 0.0, [[0.0, 0.0], [0.0, 0.0]]), (0.0, 0.0, [[0.0, 0.0], [0.0, 0.0]])], dtype=[('x', '>f4'), ('y', '>f4'), ('value', '>f4', (2, 2))])
4) Dictionary argument: two different forms are permitted. The ﬁrst consists of a dictionary with two required keys (‘names’ and ‘formats’), each having an equal sized list of values. The format list contains any type/shape speciﬁer allowed in other contexts. The names must be strings. There are two optional keys: ‘offsets’ and ‘titles’. Each must be a correspondingly matching list to the required two where offsets contain integer offsets for each ﬁeld, and titles are objects containing metadata for each ﬁeld (these do not have to be strings), where the value of None is permitted. As an example: >>> x = np.zeros(3, dtype={'names':['col1', 'col2'], 'formats':['i4','f4']}) >>> x array([(0, 0.0), (0, 0.0), (0, 0.0)],
dtype=[('col1', '>i4'), ('col2', '>f4')])
The other dictionary form permitted is a dictionary of name keys with tuple values specifying type, offset, and an optional title. >>> x = np.zeros(3, dtype={'col1':('i1',0,'title 1'), 'col2':('f4',1,'title 2')}) >>> x array([(0, 0.0), (0, 0.0), (0, 0.0)],
dtype=[(('title 1', 'col1'), '|i1'), (('title 2', 'col2'), '>f4')])
3.7.3 Accessing and modifying ﬁeld names
The ﬁeld names are an attribute of the dtype object deﬁning the structure. For the last example: >>> x.dtype.names ('col1', 'col2') >>> x.dtype.names = ('x', 'y') >>> x array([(0, 0.0), (0, 0.0), (0, 0.0)],
dtype=[(('title 1', 'x'), '|i1'), (('title 2', 'y'), '>f4')]) >>> x.dtype.names = ('x', 'y', 'z') # wrong number of names <type 'exceptions.ValueError'>: must replace all names at once with a sequence of length 2
3.7.4 Accessing ﬁeld titles
The ﬁeld titles provide a standard place to put associated info for ﬁelds. They do not have to be strings. >>> x.dtype.fields['x'][2] 'title 1'
3.7.5 Accessing multiple ﬁelds at once
You can access multiple ﬁelds at once using a list of ﬁeld names: >>> x = np.array([(1.5,2.5,(1.0,2.0)),(3.,4.,(4.,5.)),(1.,3.,(2.,6.))],
dtype=[('x','f4'),('y',np.float32),('value','f4',(2,2))])
52 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0
Notice that x is created with a list of tuples.
>>> x[['x','y']] array([(1.5, 2.5), (3.0, 4.0), (1.0, 3.0)],
dtype=[('x', '<f4'), ('y', '<f4')]) >>> x[['x','value']] array([(1.5, [[1.0, 2.0], [1.0, 2.0]]), (3.0, [[4.0, 5.0], [4.0, 5.0]]),
(1.0, [[2.0, 6.0], [2.0, 6.0]])], dtype=[('x', '<f4'), ('value', '<f4', (2, 2))])
The ﬁelds are returned in the order they are asked for.:
>>> x[['y','x']] array([(2.5, 1.5), (4.0, 3.0), (3.0, 1.0)],
dtype=[('y', '<f4'), ('x', '<f4')])
3.7.6 Filling structured arrays
Structured arrays can be ﬁlled by ﬁeld or row by row.
>>> arr = np.zeros((5,), dtype=[('var1','f8'),('var2','f8')]) >>> arr['var1'] = np.arange(5)
If you ﬁll it in row by row, it takes a take a tuple (but not a list or array!):
>>> arr[0] = (10,20) >>> arr array([(10.0, 20.0), (1.0, 0.0), (2.0, 0.0), (3.0, 0.0), (4.0, 0.0)],
dtype=[('var1', '<f8'), ('var2', '<f8')])
3.7.7 Record Arrays
For convenience, numpy provides “record arrays” which allow one to access ﬁelds of structured arrays by attribute rather than by index. Record arrays are structured arrays wrapped using a subclass of ndarray, numpy.recarray, which allows ﬁeld access by attribute on the array object, and record arrays also use a special datatype, numpy.record, which allows ﬁeld access by attribute on the individual elements of the array. The simplest way to create a record array is with numpy.rec.array:
>>> recordarr = np.rec.array([(1,2.,'Hello'),(2,3.,"World")], ... dtype=[('foo', 'i4'),('bar', 'f4'), ('baz', 'S10')]) >>> recordarr.bar array([ 2., 3.], dtype=float32) >>> recordarr[1:2] rec.array([(2, 3.0, 'World')],
dtype=[('foo', '<i4'), ('bar', '<f4'), ('baz', 'S10')]) >>> recordarr[1:2].foo array([2], dtype=int32) >>> recordarr.foo[1:2] array([2], dtype=int32) >>> recordarr[1].baz 'World'
numpy.rec.array can convert a wide variety of arguments into record arrays, including normal structured arrays:

3.7. Structured arrays

53

NumPy User Guide, Release 1.11.0
>>> arr = array([(1,2.,'Hello'),(2,3.,"World")], ... dtype=[('foo', 'i4'), ('bar', 'f4'), ('baz', 'S10')]) >>> recordarr = np.rec.array(arr)
The numpy.rec module provides a number of other convenience functions for creating record arrays, see record array creation routines. A record array representation of a structured array can be obtained using the appropriate view: >>> arr = np.array([(1,2.,'Hello'),(2,3.,"World")], ... dtype=[('foo', 'i4'),('bar', 'f4'), ('baz', 'a10')]) >>> recordarr = arr.view(dtype=dtype((np.record, arr.dtype)), ... type=np.recarray)
For convenience, viewing an ndarray as type np.recarray will automatically convert to np.record datatype, so the dtype can be left out of the view: >>> recordarr = arr.view(np.recarray) >>> recordarr.dtype dtype((numpy.record, [('foo', '<i4'), ('bar', '<f4'), ('baz', 'S10')]))
To get back to a plain ndarray both the dtype and type must be reset. The following view does so, taking into account the unusual case that the recordarr was not a structured type: >>> arr2 = recordarr.view(recordarr.dtype.fields or recordarr.dtype, np.ndarray)
Record array ﬁelds accessed by index or by attribute are returned as a record array if the ﬁeld has a structured type but as a plain ndarray otherwise. >>> recordarr = np.rec.array([('Hello', (1,2)),("World", (3,4))], ... dtype=[('foo', 'S6'),('bar', [('A', int), ('B', int)])]) >>> type(recordarr.foo) <type 'numpy.ndarray'> >>> type(recordarr.bar) <class 'numpy.core.records.recarray'>
Note that if a ﬁeld has the same name as an ndarray attribute, the ndarray attribute takes precedence. Such ﬁelds will be inaccessible by attribute but may still be accessed by index.
3.8 Subclassing ndarray
3.8.1 Credits
This page is based with thanks on the wiki page on subclassing by Pierre Gerard-Marchant http://www.scipy.org/Subclasses.
3.8.2 Introduction
Subclassing ndarray is relatively simple, but it has some complications compared to other Python objects. On this page we explain the machinery that allows you to subclass ndarray, and the implications for implementing a subclass.
ndarrays and object creation
Subclassing ndarray is complicated by the fact that new instances of ndarray classes can come about in three different ways. These are:
54 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0

1. Explicit constructor call - as in MySubClass(params). This is the usual route to Python instance creation.
2. View casting - casting an existing ndarray as a given subclass
3. New from template - creating a new instance from a template instance. Examples include returning slices from a subclassed array, creating return types from ufuncs, and copying arrays. See Creating new from template for more details
The last two are characteristics of ndarrays - in order to support things like array slicing. The complications of subclassing ndarray are due to the mechanisms numpy has to support these latter two routes of instance creation.
3.8.3 View casting
View casting is the standard ndarray mechanism by which you take an ndarray of any subclass, and return a view of the array as another (speciﬁed) subclass:
>>> import numpy as np >>> # create a completely useless ndarray subclass >>> class C(np.ndarray): pass >>> # create a standard ndarray >>> arr = np.zeros((3,)) >>> # take a view of it, as our useless subclass >>> c_arr = arr.view(C) >>> type(c_arr) <class 'C'>

3.8.4 Creating new from template
New instances of an ndarray subclass can also come about by a very similar mechanism to View casting, when numpy ﬁnds it needs to create a new instance from a template instance. The most obvious place this has to happen is when you are taking slices of subclassed arrays. For example:
>>> v = c_arr[1:] >>> type(v) # the view is of type 'C' <class 'C'> >>> v is c_arr # but it's a new instance False
The slice is a view onto the original c_arr data. So, when we take a view from the ndarray, we return a new ndarray, of the same class, that points to the data in the original.
There are other points in the use of ndarrays where we need such views, such as copying arrays (c_arr.copy()), creating ufunc output arrays (see also __array_wrap__ for ufuncs), and reducing methods (like c_arr.mean().

3.8.5 Relationship of view casting and new-from-template
These paths both use the same machinery. We make the distinction here, because they result in different input to your methods. Speciﬁcally, View casting means you have created a new instance of your array type from any potential subclass of ndarray. Creating new from template means you have created a new instance of your class from a preexisting instance, allowing you - for example - to copy across attributes that are particular to your subclass.

3.8.6 Implications for subclassing
If we subclass ndarray, we need to deal not only with explicit construction of our array type, but also View casting or Creating new from template. Numpy has the machinery to do this, and this machinery that makes subclassing slightly

3.8. Subclassing ndarray

55

NumPy User Guide, Release 1.11.0
non-standard. There are two aspects to the machinery that ndarray uses to support views and new-from-template in subclasses. The ﬁrst is the use of the ndarray.__new__ method for the main work of object initialization, rather then the more usual __init__ method. The second is the use of the __array_finalize__ method to allow subclasses to clean up after the creation of views and new instances from templates.
A brief Python primer on __new__ and __init__
__new__ is a standard Python method, and, if present, is called before __init__ when we create a class instance. See the python __new__ documentation for more detail. For example, consider the following Python code:
class C(object): def __new__(cls, *args): print('Cls in __new__:', cls) print('Args in __new__:', args) return object.__new__(cls, *args)
def __init__(self, *args): print('type(self) in __init__:', type(self)) print('Args in __init__:', args)
meaning that we get:
>>> c = C('hello') Cls in __new__: <class 'C'> Args in __new__: ('hello',) type(self) in __init__: <class 'C'> Args in __init__: ('hello',)
When we call C(’hello’), the __new__ method gets its own class as ﬁrst argument, and the passed argument, which is the string ’hello’. After python calls __new__, it usually (see below) calls our __init__ method, with the output of __new__ as the ﬁrst argument (now a class instance), and the passed arguments following. As you can see, the object can be initialized in the __new__ method or the __init__ method, or both, and in fact ndarray does not have an __init__ method, because all the initialization is done in the __new__ method. Why use __new__ rather than just the usual __init__? Because in some cases, as for ndarray, we want to be able to return an object of some other class. Consider the following:
class D(C): def __new__(cls, *args): print('D cls is:', cls) print('D args in __new__:', args) return C.__new__(C, *args)
def __init__(self, *args): # we never get here print('In D __init__')
meaning that:
>>> obj = D('hello') D cls is: <class 'D'> D args in __new__: ('hello',) Cls in __new__: <class 'C'> Args in __new__: ('hello',)
56 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0

>>> type(obj) <class 'C'>
The deﬁnition of C is the same as before, but for D, the __new__ method returns an instance of class C rather than D. Note that the __init__ method of D does not get called. In general, when the __new__ method returns an object of class other than the class in which it is deﬁned, the __init__ method of that class is not called.
This is how subclasses of the ndarray class are able to return views that preserve the class type. When taking a view, the standard ndarray machinery creates the new ndarray object with something like:
obj = ndarray.__new__(subtype, shape, ...
where subdtype is the subclass. Thus the returned view is of the same class as the subclass, rather than being of class ndarray.
That solves the problem of returning views of the same type, but now we have a new problem. The machinery of ndarray can set the class this way, in its standard methods for taking views, but the ndarray __new__ method knows nothing of what we have done in our own __new__ method in order to set attributes, and so on. (Aside - why not call obj = subdtype.__new__(... then? Because we may not have a __new__ method with the same call signature).
The role of __array_finalize__
__array_finalize__ is the mechanism that numpy provides to allow subclasses to handle the various ways that new instances get created.
Remember that subclass instances can come about in these three ways:
1. explicit constructor call (obj = MySubClass(params)). This will call the usual sequence of MySubClass.__new__ then (if it exists) MySubClass.__init__.
2. View casting
3. Creating new from template
Our MySubClass.__new__ method only gets called in the case of the explicit constructor call, so we can’t rely on MySubClass.__new__ or MySubClass.__init__ to deal with the view casting and new-from-template. It turns out that MySubClass.__array_finalize__ does get called for all three methods of object creation, so this is where our object creation housekeeping usually goes.
• For the explicit constructor call, our subclass will need to create a new ndarray instance of its own class. In practice this means that we, the authors of the code, will need to make a call to ndarray.__new__(MySubClass,...), or do view casting of an existing array (see below)
• For view casting and new-from-template, the equivalent of ndarray.__new__(MySubClass,... is called, at the C level.
The arguments that __array_finalize__ recieves differ for the three methods of instance creation above.
The following code allows us to look at the call sequences and arguments:
import numpy as np
class C(np.ndarray): def __new__(cls, *args, **kwargs): print('In __new__ with class %s' % cls) return np.ndarray.__new__(cls, *args, **kwargs)
def __init__(self, *args, **kwargs): # in practice you probably will not need or want an __init__

3.8. Subclassing ndarray

57

NumPy User Guide, Release 1.11.0
# method for your subclass print('In __init__ with class %s' % self.__class__)
def __array_finalize__(self, obj): print('In array_finalize:') print(' self type is %s' % type(self)) print(' obj type is %s' % type(obj))
Now:
>>> # Explicit constructor >>> c = C((10,)) In __new__ with class <class 'C'> In array_finalize:
self type is <class 'C'> obj type is <type 'NoneType'> In __init__ with class <class 'C'> >>> # View casting >>> a = np.arange(10) >>> cast_a = a.view(C) In array_finalize: self type is <class 'C'> obj type is <type 'numpy.ndarray'> >>> # Slicing (example of new-from-template) >>> cv = c[:1] In array_finalize: self type is <class 'C'> obj type is <class 'C'>
The signature of __array_finalize__ is:
def __array_finalize__(self, obj):
ndarray.__new__ passes __array_finalize__ the new object, of our own class (self) as well as the object from which the view has been taken (obj). As you can see from the output above, the self is always a newly created instance of our subclass, and the type of obj differs for the three instance creation methods:
• When called from the explicit constructor, obj is None • When called from view casting, obj can be an instance of any subclass of ndarray, including our own. • When called in new-from-template, obj is another instance of our own subclass, that we might use to update
the new self instance. Because __array_finalize__ is the only method that always sees new instances being created, it is the sensible place to ﬁll in instance defaults for new object attributes, among other tasks. This may be clearer with an example.
3.8.7 Simple example - adding an extra attribute to ndarray
import numpy as np
class InfoArray(np.ndarray):
def __new__(subtype, shape, dtype=float, buffer=None, offset=0, strides=None, order=None, info=None):
# Create the ndarray instance of our type, given the usual # ndarray input arguments. This will call the standard
58 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0

# ndarray constructor, but return an object of our type. # It also triggers a call to InfoArray.__array_finalize__ obj = np.ndarray.__new__(subtype, shape, dtype, buffer, offset, strides,
order) # set the new 'info' attribute to the value passed obj.info = info # Finally, we must return the newly created object: return obj
def __array_finalize__(self, obj): # ``self`` is a new object resulting from # ndarray.__new__(InfoArray, ...), therefore it only has # attributes that the ndarray.__new__ constructor gave it # i.e. those of a standard ndarray. # # We could have got to the ndarray.__new__ call in 3 ways: # From an explicit constructor - e.g. InfoArray(): # obj is None # (we're in the middle of the InfoArray.__new__ # constructor, and self.info will be set when we return to # InfoArray.__new__) if obj is None: return # From view casting - e.g arr.view(InfoArray): # obj is arr # (type(obj) can be InfoArray) # From new-from-template - e.g infoarr[:3] # type(obj) is InfoArray # # Note that it is here, rather than in the __new__ method, # that we set the default value for 'info', because this # method sees all creation of default objects - with the # InfoArray.__new__ constructor, but also with # arr.view(InfoArray). self.info = getattr(obj, 'info', None) # We do not need to return anything
Using the object looks like this:
>>> obj = InfoArray(shape=(3,)) # explicit constructor >>> type(obj) <class 'InfoArray'> >>> obj.info is None True >>> obj = InfoArray(shape=(3,), info='information') >>> obj.info 'information' >>> v = obj[1:] # new-from-template - here - slicing >>> type(v) <class 'InfoArray'> >>> v.info 'information' >>> arr = np.arange(10) >>> cast_arr = arr.view(InfoArray) # view casting >>> type(cast_arr) <class 'InfoArray'> >>> cast_arr.info is None True
This class isn’t very useful, because it has the same constructor as the bare ndarray object, including passing in buffers

3.8. Subclassing ndarray

59

NumPy User Guide, Release 1.11.0
and shapes and so on. We would probably prefer the constructor to be able to take an already formed ndarray from the usual numpy calls to np.array and return an object.
3.8.8 Slightly more realistic example - attribute added to existing array
Here is a class that takes a standard ndarray that already exists, casts as our type, and adds an extra attribute. import numpy as np
class RealisticInfoArray(np.ndarray):
def __new__(cls, input_array, info=None): # Input array is an already formed ndarray instance # We first cast to be our class type obj = np.asarray(input_array).view(cls) # add the new attribute to the created instance obj.info = info # Finally, we must return the newly created object: return obj
def __array_finalize__(self, obj): # see InfoArray.__array_finalize__ for comments if obj is None: return self.info = getattr(obj, 'info', None)
So: >>> arr = np.arange(5) >>> obj = RealisticInfoArray(arr, info='information') >>> type(obj) <class 'RealisticInfoArray'> >>> obj.info 'information' >>> v = obj[1:] >>> type(v) <class 'RealisticInfoArray'> >>> v.info 'information'
3.8.9 __array_wrap__ for ufuncs
__array_wrap__ gets called at the end of numpy ufuncs and other numpy functions, to allow a subclass to set the type of the return value and update attributes and metadata. Let’s show how this works with an example. First we make the same subclass as above, but with a different name and some print statements: import numpy as np
class MySubClass(np.ndarray):
def __new__(cls, input_array, info=None): obj = np.asarray(input_array).view(cls) obj.info = info return obj
def __array_finalize__(self, obj): print('In __array_finalize__:')
60 Chapter 3. Numpy basics

NumPy User Guide, Release 1.11.0

print(' self is %s' % repr(self)) print(' obj is %s' % repr(obj)) if obj is None: return self.info = getattr(obj, 'info', None)
def __array_wrap__(self, out_arr, context=None): print('In __array_wrap__:') print(' self is %s' % repr(self)) print(' arr is %s' % repr(out_arr)) # then just call the parent return np.ndarray.__array_wrap__(self, out_arr, context)
We run a ufunc on an instance of our new array:
>>> obj = MySubClass(np.arange(5), info='spam') In __array_finalize__:
self is MySubClass([0, 1, 2, 3, 4]) obj is array([0, 1, 2, 3, 4]) >>> arr2 = np.arange(5)+1 >>> ret = np.add(arr2, obj) In __array_wrap__: self is MySubClass([0, 1, 2, 3, 4]) arr is array([1, 3, 5, 7, 9]) In __array_finalize__: self is MySubClass([1, 3, 5, 7, 9]) obj is MySubClass([0, 1, 2, 3, 4]) >>> ret MySubClass([1, 3, 5, 7, 9]) >>> ret.info 'spam'
Note that the ufunc (np.add) has called the __array_wrap__ method of the input with the highest __array_priority__ value, in this case MySubClass.__array_wrap__, with arguments self as obj, and out_arr as the (ndarray) result of the addition. In turn, the default __array_wrap__ (ndarray.__array_wrap__) has cast the result to class MySubClass, and called __array_finalize__ hence the copying of the info attribute. This has all happened at the C level.
But, we could do anything we wanted:
class SillySubClass(np.ndarray):
def __array_wrap__(self, arr, context=None): return 'I lost your data'
>>> arr1 = np.arange(5) >>> obj = arr1.view(SillySubClass) >>> arr2 = np.arange(5) >>> ret = np.multiply(obj, arr2) >>> ret 'I lost your data'
So, by deﬁning a speciﬁc __array_wrap__ method for our subclass, we can tweak the output from ufuncs. The __array_wrap__ method requires self, then an argument - which is the result of the ufunc - and an optional parameter context. This parameter is returned by some ufuncs as a 3-element tuple: (name of the ufunc, argument of the ufunc, domain of the ufunc). __array_wrap__ should return an instance of its containing class. See the masked array subclass for an implementation.
In addition to __array_wrap__, which is called on the way out of the ufunc, there is also an __array_prepare__ method which is called on the way into the ufunc, after the output arrays are created but

3.8. Subclassing ndarray

61

NumPy User Guide, Release 1.11.0
before any computation has been performed. The default implementation does nothing but pass through the array. __array_prepare__ should not attempt to access the array data or resize the array, it is intended for setting the output array type, updating attributes and metadata, and performing any checks based on the input that may be desired before computation begins. Like __array_wrap__, __array_prepare__ must return an ndarray or subclass thereof or raise an error.
3.8.10 Extra gotchas - custom __del__ methods and ndarray.base
One of the problems that ndarray solves is keeping track of memory ownership of ndarrays and their views. Consider the case where we have created an ndarray, arr and have taken a slice with v = arr[1:]. The two objects are looking at the same memory. Numpy keeps track of where the data came from for a particular array or view, with the base attribute: >>> # A normal ndarray, that owns its own data >>> arr = np.zeros((4,)) >>> # In this case, base is None >>> arr.base is None True >>> # We take a view >>> v1 = arr[1:] >>> # base now points to the array that it derived from >>> v1.base is arr True >>> # Take a view of a view >>> v2 = v1[1:] >>> # base points to the view it derived from >>> v2.base is v1 True In general, if the array owns its own memory, as for arr in this case, then arr.base will be None - there are some exceptions to this - see the numpy book for more details. The base attribute is useful in being able to tell whether we have a view or the original array. This in turn can be useful if we need to know whether or not to do some speciﬁc cleanup when the subclassed array is deleted. For example, we may only want to do the cleanup if the original array is deleted, but not the views. For an example of how this can work, have a look at the memmap class in numpy.core.
62 Chapter 3. Numpy basics

CHAPTER
FOUR
MISCELLANEOUS
4.1 IEEE 754 Floating Point Special Values
Special values deﬁned in numpy: nan, inf, NaNs can be used as a poor-man’s mask (if you don’t care what the original value was) Note: cannot use equality to test NaNs. E.g.: >>> myarr = np.array([1., 0., np.nan, 3.]) >>> np.where(myarr == np.nan) >>> np.nan == np.nan # is always False! Use special numpy functions instead. False >>> myarr[myarr == np.nan] = 0. # doesn't work >>> myarr array([ 1., 0., NaN, 3.]) >>> myarr[np.isnan(myarr)] = 0. # use this instead find >>> myarr array([ 1., 0., 0., 3.])
Other related special value functions: isinf(): True if value is inf isfinite(): True if not nan or inf nan_to_num(): Map nan to 0, inf to max float, -inf to min float
The following corresponds to the usual functions except that nans are excluded from the results: nansum() nanmax() nanmin() nanargmax() nanargmin()
>>> x = np.arange(10.) >>> x[3] = np.nan >>> x.sum() nan >>> np.nansum(x) 42.0
63

NumPy User Guide, Release 1.11.0
4.2 How numpy handles numerical exceptions
The default is to ’warn’ for invalid, divide, and overflow and ’ignore’ for underflow. But this can be changed, and it can be set individually for different kinds of exceptions. The different behaviors are:
• ‘ignore’ : Take no action when the exception occurs. • ‘warn’ : Print a RuntimeWarning (via the Python warnings module). • ‘raise’ : Raise a FloatingPointError. • ‘call’ : Call a function speciﬁed using the seterrcall function. • ‘print’ : Print a warning directly to stdout. • ‘log’ : Record error in a Log object speciﬁed by seterrcall. These behaviors can be set for all kinds of errors or speciﬁc ones: • all : apply to all numeric exceptions • invalid : when NaNs are generated • divide : divide by zero (for integers as well!) • overﬂow : ﬂoating point overﬂows • underﬂow : ﬂoating point underﬂows Note that integer divide-by-zero is handled by the same machinery. These behaviors are set on a per-thread basis.
4.3 Examples
>>> oldsettings = np.seterr(all='warn') >>> np.zeros(5,dtype=np.float32)/0. invalid value encountered in divide >>> j = np.seterr(under='ignore') >>> np.array([1.e-100])**10 >>> j = np.seterr(invalid='raise') >>> np.sqrt(np.array([-1.])) FloatingPointError: invalid value encountered in sqrt >>> def errorhandler(errstr, errflag): ... print("saw stupid error!") >>> np.seterrcall(errorhandler) <function err_handler at 0x...> >>> j = np.seterr(all='call') >>> np.zeros(5, dtype=np.int32)/0 FloatingPointError: invalid value encountered in divide saw stupid error! >>> j = np.seterr(**oldsettings) # restore previous ... # error-handling settings

4.4 Interfacing to C
Only a survey of the choices. Little detail on how each works. 1. Bare metal, wrap your own C-code manually. • Plusses:
64

Chapter 4. Miscellaneous

NumPy User Guide, Release 1.11.0

– Efﬁcient

– No dependencies on other tools

• Minuses:

– Lots of learning overhead:

* need to learn basics of Python C API * need to learn basics of numpy C API * need to learn how to handle reference counting and love it. – Reference counting often difﬁcult to get right.

* getting it wrong leads to memory leaks, and worse, segfaults – API will change for Python 3.0!

2. Cython

• Plusses:

– avoid learning C API’s

– no dealing with reference counting

– can code in pseudo python and generate C code

– can also interface to existing C code

– should shield you from changes to Python C api

– has become the de-facto standard within the scientiﬁc Python community

– fast indexing support for arrays

• Minuses:

– Can write code in non-standard form which may become obsolete

– Not as ﬂexible as manual wrapping

3. ctypes

• Plusses:

– part of Python standard library

– good for interfacing to existing sharable libraries, particularly Windows DLLs

– avoids API/reference counting issues

– good numpy support: arrays have all these in their ctypes attribute:

a.ctypes.data a.ctypes.data_as a.ctypes.get_as_parameter a.ctypes.get_data a.ctypes.get_shape

a.ctypes.get_strides a.ctypes.shape a.ctypes.shape_as a.ctypes.strides a.ctypes.strides_as

• Minuses: – can’t use for writing code to be turned into C extensions, only a wrapper tool.
4. SWIG (automatic wrapper generator) • Plusses:

4.4. Interfacing to C

65

NumPy User Guide, Release 1.11.0
– around a long time – multiple scripting language support – C++ support – Good for wrapping large (many functions) existing C libraries • Minuses: – generates lots of code between Python and the C code – can cause performance problems that are nearly impossible to optimize out – interface ﬁles can be hard to write – doesn’t necessarily avoid reference counting issues or needing to know API’s 5. scipy.weave • Plusses: – can turn many numpy expressions into C code – dynamic compiling and loading of generated C code – can embed pure C code in Python module and have weave extract, generate interfaces and compile, etc. • Minuses: – Future very uncertain: it’s the only part of Scipy not ported to Python 3 and is effectively deprecated in
favor of Cython. 6. Psyco • Plusses:
– Turns pure python into efﬁcient machine code through jit-like optimizations – very fast when it optimizes well • Minuses: – Only on intel (windows?) – Doesn’t do much for numpy?
4.5 Interfacing to Fortran:
The clear choice to wrap Fortran code is f2py. Pyfort is an older alternative, but not supported any longer. Fwrap is a newer project that looked promising but isn’t being developed any longer.
4.6 Interfacing to C++:
1. Cython 2. CXX 3. Boost.python 4. SWIG
66 Chapter 4. Miscellaneous

5. SIP (used mainly in PyQT)

NumPy User Guide, Release 1.11.0

4.6. Interfacing to C++:

67

NumPy User Guide, Release 1.11.0 68 Chapter 4. Miscellaneous

CHAPTER
FIVE
NUMPY FOR MATLAB USERS

5.1 Introduction
MATLAB® and NumPy/SciPy have a lot in common. But there are many differences. NumPy and SciPy were created to do numerical and scientiﬁc computing in the most natural way with Python, not to be MATLAB® clones. This page is intended to be a place to collect wisdom about the differences, mostly for the purpose of helping proﬁcient MATLAB® users become proﬁcient NumPy and SciPy users.

5.2 Some Key Differences

In MATLAB®, the basic data type is a multidimensional array of double precision ﬂoating point numbers. Most expressions take such arrays and return such arrays. Operations on the 2-D instances of these arrays are designed to act more or less like matrix operations in linear algebra. MATLAB® uses 1 (one) based indexing. The initial element of a sequence is found using a(1). See note INDEXING MATLAB®’s scripting language was created for doing linear algebra. The syntax for basic matrix operations is nice and clean, but the API for adding GUIs and making full-ﬂedged applications is more or less an afterthought.
In MATLAB®, arrays have pass-by-value semantics, with a lazy copy-on-write scheme to prevent actually creating copies until they are actually needed. Slice operations copy parts of the array.

In NumPy the basic type is a multidimensional array. Operations on these arrays in all dimensionalities including 2D are elementwise operations. However, there is a special matrix type for doing linear algebra, which is just a subclass of the array class. Operations on matrix-class arrays are linear algebra operations. Python uses 0 (zero) based indexing. The initial element of a sequence is found using a[0].
NumPy is based on Python, which was designed from the outset to be an excellent general-purpose programming language. While Matlab’s syntax for some array manipulations is more compact than NumPy’s, NumPy (by virtue of being an add-on to Python) can do many things that Matlab just cannot, for instance subclassing the main array type to do both array and matrix math cleanly. In NumPy arrays have pass-by-reference semantics. Slice operations are views into an array.

5.3 ‘array’ or ‘matrix’? Which should I use?
Numpy provides, in addition to np.ndarray, an additional matrix type that you may see used in some existing code. Which one to use?

69

NumPy User Guide, Release 1.11.0

5.3.1 Short answer
Use arrays. • They are the standard vector/matrix/tensor type of numpy. Many numpy functions return arrays, not matrices. • There is a clear distinction between element-wise operations and linear algebra operations. • You can have standard vectors or row/column vectors if you like.
The only disadvantage of using the array type is that you will have to use dot instead of * to multiply (reduce) two tensors (scalar product, matrix vector multiplication etc.).

5.3.2 Long answer

Numpy contains both an array class and a matrix class. The array class is intended to be a general-purpose n-dimensional array for many kinds of numerical computing, while matrix is intended to facilitate linear algebra computations speciﬁcally. In practice there are only a handful of key differences between the two.

• Operator *, dot(), and multiply():

– For array, ‘‘‘*‘‘’ means element-wise multiplication, and the dot() function is used for matrix multiplication.

– For matrix, ‘‘‘*‘‘’ means matrix multiplication, and the multiply() function is used for elementwise multiplication.

• Handling of vectors (one-dimensional arrays)

– For array, the vector shapes 1xN, Nx1, and N are all different things. Operations like A[:,1] return a one-dimensional array of shape N, not a two-dimensional array of shape Nx1. Transpose on a one-dimensional array does nothing.

– For matrix, one-dimensional arrays are always upconverted to 1xN or Nx1 matrices (row or column vectors). A[:,1] returns a two-dimensional matrix of shape Nx1.

• Handling of higher-dimensional arrays (ndim > 2)

– array objects can have number of dimensions > 2;

– matrix objects always have exactly two dimensions.

• Convenience attributes

– array has a .T attribute, which returns the transpose of the data.

– matrix also has .H, .I, and .A attributes, which return the conjugate transpose, inverse, and asarray() of the matrix, respectively.

• Convenience constructor

– The array constructor takes (nested) Python sequences as initializers. array([[1,2,3],[4,5,6]]).

As in,

– The matrix constructor additionally takes a convenient string initializer. matrix("[1 2 3; 4 5 6]").

As in

There are pros and cons to using both:

• array

– :) You can treat one-dimensional arrays as either row or column vectors. dot(A,v) treats v as a column vector, while dot(v,A) treats v as a row vector. This can save you having to type a lot of transposes.

70 Chapter 5. Numpy for Matlab users

NumPy User Guide, Release 1.11.0

– <:( Having to use the dot() function for matrix-multiply is messy – dot(dot(A,B),C) vs. A*B*C. – :) Element-wise multiplication is easy: A*B. – :) array is the “default” NumPy type, so it gets the most testing, and is the type most likely to be
returned by 3rd party code that uses NumPy.
– :) Is quite at home handling data of any number of dimensions.
– :) Closer in semantics to tensor algebra, if you are familiar with that.
– :) All operations (*, /, +, - etc.) are elementwise • matrix
– :\\ Behavior is more like that of MATLAB® matrices.
– <:( Maximum of two-dimensional. To hold three-dimensional data you need array or perhaps a Python list of matrix.
– <:( Minimum of two-dimensional. You cannot have vectors. They must be cast as single-column or single-row matrices.
– <:( Since array is the default in NumPy, some functions may return an array even if you give them a matrix as an argument. This shouldn’t happen with NumPy functions (if it does it’s a bug), but 3rd party code based on NumPy may not honor type preservation like NumPy does.
– :) A*B is matrix multiplication, so more convenient for linear algebra. – <:( Element-wise multiplication requires calling a function, multipy(A,B).
– <:( The use of operator overloading is a bit illogical: * does not work elementwise but / does. The array is thus much more advisable to use.

5.4 Facilities for Matrix Users
Numpy has some features that facilitate the use of the matrix type, which hopefully make things easier for Matlab converts.
• A matlib module has been added that contains matrix versions of common array constructors like ones(), zeros(), empty(), eye(), rand(), repmat(), etc. Normally these functions return arrays, but the matlib versions return matrix objects.
• mat has been changed to be a synonym for asmatrix, rather than matrix, thus making it a concise way to convert an array to a matrix without copying the data.
• Some top-level functions have been removed. For example numpy.rand() now needs to be accessed as numpy.random.rand(). Or use the rand() from the matlib module. But the “numpythonic” way is to use numpy.random.random(), which takes a tuple for the shape, like other numpy functions.

5.5 Table of Rough MATLAB-NumPy Equivalents

The table below gives rough equivalents for some common MATLAB® expressions. These are not exact equivalents, but rather should be taken as hints to get you going in the right direction. For more detail read the built-in documentation on the NumPy functions.
Some care is necessary when writing functions that take arrays or matrices as arguments — if you are expecting an array and are given a matrix, or vice versa, then ‘*’ (multiplication) will give you unexpected results. You can convert back and forth between arrays and matrices using

5.4. Facilities for Matrix Users

71

NumPy User Guide, Release 1.11.0

• asarray: always returns an object of type array • asmatrix or mat: always return an object of type matrix • asanyarray: always returns an array object or a subclass derived from it, depending on the input. For
instance if you pass in a matrix it returns a matrix. These functions all accept both arrays and matrices (among other things like Python lists), and thus are useful when writing functions that should accept any array-like object. In the table below, it is assumed that you have executed the following commands in Python:
from numpy import * import scipy.linalg
Also assume below that if the Notes talk about “matrix” that the arguments are two-dimensional entities.

5.5.1 General Purpose Equivalents

MATLAB help func which func type func a && b
a || b
1*i, 1*j, 1i, 1j eps
ode45 ode15s

numpy info(func) or help(func) or func? (in Ipython) see note HELP source(func) or func?? (in Ipython) a and b
a or b 1j

Notes
get help on the function func
ﬁnd out where func is deﬁned
print source for func (if not a native function)
short-circuiting logical AND operator (Python native operator); scalar arguments only short-circuiting logical OR operator (Python native operator); scalar arguments only complex numbers

np.spacing(1)

Distance between 1 and the nearest ﬂoating

point number.

scipy.integrate.ode(f).set_integrator(i’ntdeogrparteia5n’O)DE with Runge-Kutta 4,5

scipy.integrate.ode(f).set_integrator(i’ntveogrdaete’a,n ODE with BDF method

method=’bdf’, order=5)

5.5.2 Linear Algebra Equivalents

MATLAB ndims(a) numel(a) size(a) size(a,n) [ 1 2 3; 4 5 6 ] [ a b; c d ] a(end) a(2,5) a(2,:)

NumPy ndim(a) or a.ndim size(a) or a.size shape(a) or a.shape a.shape[n-1] array([[1.,2.,3.], [4.,5.,6.]]) vstack([hstack([a,b]), hstack([c,d])]) or bmat(’a b; c d’) a[-1] a[1,4] a[1] or a[1,:]

72 Chapter 5. Numpy for Matlab users

NumPy User Guide, Release 1.11.0

MATLAB a(1:5,:) a(end-4:end,:) a(1:3,5:9) a([2,4,5],[1,3]) a(3:2:21,:) a(1:2:end,:) a(end:-1:1,:) or flipud(a) a([1:end 1],:) a.’ a’ a*b a .* b a./b a.^3 (a>0.5) find(a>0.5) a(:,find(v>0.5)) a(:,find(v>0.5)) a(a<0.5)=0 a .* (a>0.5) a(:) = 3 y=x y=x(2,:) y=x(:) 1:10 0:9 [1:10]’ zeros(3,4) zeros(3,4,5) ones(3,4) eye(3) diag(a) diag(a,0) rand(3,4) linspace(1,3,4) [x,y]=meshgrid(0:8,0:5)
[x,y]=meshgrid([1,2,4],[2,4,5])
repmat(a, m, n) [a b] [a; b] max(max(a)) max(a) max(a,[],2) max(a,b) norm(v) a&b a|b bitand(a,b) bitor(a,b)

NumPy a[0:5] or a[:5] or a[0:5,:] a[-5:] a[0:3][:,4:9] a[ix_([1,3,4],[0,2])] a[ 2:21:2,:] a[ ::2,:] a[ ::-1,:] a[r_[:len(a),0]] a.transpose() or a.T a.conj().transpose() or a.conj().T a.dot(b) a*b a/b a**3 (a>0.5) nonzero(a>0.5) a[:,nonzero(v>0.5)[0]] a[:,v.T>0.5] a[a<0.5]=0 a * (a>0.5) a[:] = 3 y = x.copy() y = x[1,:].copy() y = x.flatten() arange(1.,11.) or r_[1.:11.] or r_[1:10:10j] arange(10.) or r_[:10.] or r_[:9:10j] arange(1.,11.)[:, newaxis] zeros((3,4)) zeros((3,4,5)) ones((3,4)) eye(3) diag(a) diag(a,0) random.rand(3,4) linspace(1,3,4) mgrid[0:9.,0:6.] or meshgrid(r_[0:9.],r_[0:6.] ogrid[0:9.,0:6.] or ix_(r_[0:9.],r_[0:6.] meshgrid([1,2,4],[2,4,5]) ix_([1,2,4],[2,4,5]) tile(a, (m, n)) concatenate((a,b),1) or hstack((a,b)) or column_stack((a,b)) concatenate((a,b)) or vstack((a,b)) or r_[a,b] a.max() a.max(0) a.max(1) maximum(a, b) sqrt(dot(v,v)) or np.linalg.norm(v) logical_and(a,b) logical_or(a,b) a&b a|b

5.5. Table of Rough MATLAB-NumPy Equivalents

73

NumPy User Guide, Release 1.11.0

MATLAB inv(a) pinv(a) rank(a) a\b b/a [U,S,V]=svd(a) chol(a) [V,D]=eig(a) [V,D]=eig(a,b) [V,D]=eigs(a,k) [Q,R,P]=qr(a,0) [L,U,P]=lu(a) conjgrad fft(a) ifft(a) sort(a) [b,I] = sortrows(a,i) regress(y,X) decimate(x, q) unique(a) squeeze(a)

NumPy linalg.inv(a) linalg.pinv(a) linalg.matrix_rank(a) linalg.solve(a,b) if a is square; linalg.lstsq(a,b) otherwise Solve a.T x.T = b.T instead U, S, Vh = linalg.svd(a), V = Vh.T linalg.cholesky(a).T D,V = linalg.eig(a) V,D = np.linalg.eig(a,b)
Q,R = scipy.linalg.qr(a) L,U = scipy.linalg.lu(a) or LU,P=scipy.linalg.lu_factor(a) scipy.sparse.linalg.cg fft(a) ifft(a) sort(a) or a.sort() I = argsort(a[:,i]), b=a[I,:] linalg.lstsq(X,y) scipy.signal.resample(x, len(x)/q) unique(a) a.squeeze()

5.6 Notes
Submatrix: Assignment to a submatrix can be done with lists of indexes using the ix_ command. E.g., for 2d array a, one might do: ind=[1,3]; a[np.ix_(ind,ind)]+=100.
HELP: There is no direct equivalent of MATLAB’s which command, but the commands help and source will usually list the ﬁlename where the function is located. Python also has an inspect module (do import inspect) which provides a getfile that often works.
INDEXING: MATLAB® uses one based indexing, so the initial element of a sequence has index 1. Python uses zero based indexing, so the initial element of a sequence has index 0. Confusion and ﬂamewars arise because each has advantages and disadvantages. One based indexing is consistent with common human language usage, where the “ﬁrst” element of a sequence has index 1. Zero based indexing simpliﬁes indexing. See also a text by prof.dr. Edsger W. Dijkstra.
RANGES: In MATLAB®, 0:5 can be used as both a range literal and a ‘slice’ index (inside parentheses); however, in Python, constructs like 0:5 can only be used as a slice index (inside square brackets). Thus the somewhat quirky r_ object was created to allow numpy to have a similarly terse range construction mechanism. Note that r_ is not called like a function or a constructor, but rather indexed using square brackets, which allows the use of Python’s slice syntax in the arguments.
LOGICOPS: & or | in Numpy is bitwise AND/OR, while in Matlab & and | are logical AND/OR. The difference should be clear to anyone with signiﬁcant programming experience. The two can appear to work the same, but there are important differences. If you would have used Matlab’s & or | operators, you should use the Numpy ufuncs logical_and/logical_or. The notable differences between Matlab’s and Numpy’s & and | operators are:
• Non-logical {0,1} inputs: Numpy’s output is the bitwise AND of the inputs. Matlab treats any non-zero value as 1 and returns the logical AND. For example (3 & 4) in Numpy is 0, while in Matlab both 3 and 4 are considered logical true and (3 & 4) returns 1.
• Precedence: Numpy’s & operator is higher precedence than logical operators like < and >; Matlab’s is the reverse.
74 Chapter 5. Numpy for Matlab users

NumPy User Guide, Release 1.11.0
If you know you have boolean arguments, you can get away with using Numpy’s bitwise operators, but be careful with parentheses, like this: z = (x > 1) & (x < 2). The absence of Numpy operator forms of logical_and and logical_or is an unfortunate consequence of Python’s design.
RESHAPE and LINEAR INDEXING: Matlab always allows multi-dimensional arrays to be accessed using scalar or linear indices, Numpy does not. Linear indices are common in Matlab programs, e.g. ﬁnd() on a matrix returns them, whereas Numpy’s ﬁnd behaves differently. When converting Matlab code it might be necessary to ﬁrst reshape a matrix to a linear sequence, perform some indexing operations and then reshape back. As reshape (usually) produces views onto the same storage, it should be possible to do this fairly efﬁciently. Note that the scan order used by reshape in Numpy defaults to the ‘C’ order, whereas Matlab uses the Fortran order. If you are simply converting to a linear sequence and back this doesn’t matter. But if you are converting reshapes from Matlab code which relies on the scan order, then this Matlab code: z = reshape(x,3,4); should become z = x.reshape(3,4,order=’F’).copy() in Numpy.
5.7 Customizing Your Environment
In MATLAB® the main tool available to you for customizing the environment is to modify the search path with the locations of your favorite functions. You can put such customizations into a startup script that MATLAB will run on startup.
NumPy, or rather Python, has similar facilities.
• To modify your Python search path to include the locations of your own modules, deﬁne the PYTHONPATH environment variable.
• To have a particular script ﬁle executed when the interactive Python interpreter is started, deﬁne the PYTHONSTARTUP environment variable to contain the name of your startup script.
Unlike MATLAB®, where anything on your path can be called immediately, with Python you need to ﬁrst do an ‘import’ statement to make functions in a particular ﬁle accessible.
For example you might make a startup script that looks like this (Note: this is just an example, not a statement of “best practices”):
# Make all numpy available via shorter 'num' prefix import numpy as num # Make all matlib functions accessible at the top level via M.func() import numpy.matlib as M # Make some matlib functions accessible directly at the top level via, e.g. rand(3,3) from numpy.matlib import rand,zeros,ones,empty,eye # Define a Hermitian function def hermitian(A, **kwargs):
return num.transpose(A,**kwargs).conj() # Make some shorcuts for transpose,hermitian: # num.transpose(A) --> T(A) # hermitian(A) --> H(A) T = num.transpose H = hermitian
5.8 Links
See http://mathesaurus.sf.net/ for another MATLAB®/NumPy cross-reference.
An extensive list of tools for scientiﬁc work with python can be found in the topical software page.
MATLAB® and SimuLink® are registered trademarks of The MathWorks.

5.7. Customizing Your Environment

75

NumPy User Guide, Release 1.11.0 76 Chapter 5. Numpy for Matlab users

CHAPTER
SIX
BUILDING FROM SOURCE

A general overview of building NumPy from source is given here, with detailed instructions for speciﬁc platforms given seperately.

6.1 Prerequisites
Building NumPy requires the following software installed:
1. Python 2.6.x, 2.7.x, 3.2.x or newer
On Debian and derivatives (Ubuntu): python, python-dev (or python3-dev)
On Windows: the ofﬁcial python installer at www.python.org is enough
Make sure that the Python package distutils is installed before continuing. For example, in Debian GNU/Linux, installing python-dev also installs distutils.
Python must also be compiled with the zlib module enabled. This is practically always the case with prepackaged Pythons.
2. Compilers
To build any extension modules for Python, you’ll need a C compiler. Various NumPy modules use FORTRAN 77 libraries, so you’ll also need a FORTRAN 77 compiler installed.
Note that NumPy is developed mainly using GNU compilers. Compilers from other vendors such as Intel, Absoft, Sun, NAG, Compaq, Vast, Porland, Lahey, HP, IBM, Microsoft are only supported in the form of community feedback, and may not work out of the box. GCC 4.x (and later) compilers are recommended.
3. Linear Algebra libraries
NumPy does not require any external linear algebra libraries to be installed. However, if these are available, NumPy’s setup script can detect them and use them for building. A number of different LAPACK library setups can be used, including optimized LAPACK libraries such as ATLAS, MKL or the Accelerate/vecLib framework on OS X.
4. Cython
To build development versions of Numpy, you’ll need a recent version of Cython. Released Numpy sources on PyPi include the C ﬁles generated from Cython code, so for released versions having Cython installed isn’t needed.

6.2 Basic Installation
To install NumPy run:

77

NumPy User Guide, Release 1.11.0
python setup.py install To perform an in-place build that can be run from the source folder run: python setup.py build_ext --inplace The NumPy build system uses setuptools (from numpy 1.11.0, before that it was plain distutils) and numpy.distutils. Using virtualenv should work as expected. Note: for build instructions to do development work on NumPy itself, see development-environment.
6.2.1 Parallel builds
From NumPy 1.10.0 on it’s also possible to do a parallel build with: python setup.py build -j 4 install --prefix $HOME/.local This will compile numpy on 4 CPUs and install it into the speciﬁed preﬁx. to perform a parallel in-place build, run: python setup.py build_ext --inplace -j 4 The number of build jobs can also be speciﬁed via the environment variable NPY_NUM_BUILD_JOBS.
6.3 FORTRAN ABI mismatch
The two most popular open source fortran compilers are g77 and gfortran. Unfortunately, they are not ABI compatible, which means that concretely you should avoid mixing libraries built with one with another. In particular, if your blas/lapack/atlas is built with g77, you must use g77 when building numpy and scipy; on the contrary, if your atlas is built with gfortran, you must build numpy/scipy with gfortran. This applies for most other cases where different FORTRAN compilers might have been used.
6.3.1 Choosing the fortran compiler
To build with gfortran: python setup.py build --fcompiler=gnu95 For more information see: python setup.py build --help-fcompiler
6.3.2 How to check the ABI of blas/lapack/atlas
One relatively simple and reliable way to check for the compiler used to build a library is to use ldd on the library. If libg2c.so is a dependency, this means that g77 has been used. If libgfortran.so is a a dependency, gfortran has been used. If both are dependencies, this means both have been used, which is almost always a very bad idea.
6.4 Disabling ATLAS and other accelerated libraries
Usage of ATLAS and other accelerated libraries in Numpy can be disabled via:
78 Chapter 6. Building from source

NumPy User Guide, Release 1.11.0
BLAS=None LAPACK=None ATLAS=None python setup.py build
6.5 Supplying additional compiler ﬂags
Additional compiler ﬂags can be supplied by setting the OPT, FOPT (for Fortran), and CC environment variables.
6.6 Building with ATLAS support
6.6.1 Ubuntu
You can install the necessary package for optimized ATLAS with this command: sudo apt-get install libatlas-base-dev

6.5. Supplying additional compiler ﬂags

79

NumPy User Guide, Release 1.11.0 80 Chapter 6. Building from source

CHAPTER
SEVEN
USING NUMPY C-API
7.1 How to extend NumPy
That which is static and repetitive is boring. That which is dynamic and random is confusing. In between lies art. — John A. Locke
Science is a differential equation. Religion is a boundary condition. — Alan Turing
7.1.1 Writing an extension module
While the ndarray object is designed to allow rapid computation in Python, it is also designed to be general-purpose and satisfy a wide- variety of computational needs. As a result, if absolute speed is essential, there is no replacement for a well-crafted, compiled loop speciﬁc to your application and hardware. This is one of the reasons that numpy includes f2py so that an easy-to-use mechanisms for linking (simple) C/C++ and (arbitrary) Fortran code directly into Python are available. You are encouraged to use and improve this mechanism. The purpose of this section is not to document this tool but to document the more basic steps to writing an extension module that this tool depends on. When an extension module is written, compiled, and installed to somewhere in the Python path (sys.path), the code can then be imported into Python as if it were a standard python ﬁle. It will contain objects and methods that have been deﬁned and compiled in C code. The basic steps for doing this in Python are well-documented and you can ﬁnd more information in the documentation for Python itself available online at www.python.org . In addition to the Python C-API, there is a full and rich C-API for NumPy allowing sophisticated manipulations on a C-level. However, for most applications, only a few API calls will typically be used. If all you need to do is extract a pointer to memory along with some shape information to pass to another calculation routine, then you will use very different calls, then if you are trying to create a new array- like type or add a new data type for ndarrays. This chapter documents the API calls and macros that are most commonly used.
7.1.2 Required subroutine
There is exactly one function that must be deﬁned in your C-code in order for Python to use it as an extension module. The function must be called init{name} where {name} is the name of the module from Python. This function must be declared so that it is visible to code outside of the routine. Besides adding the methods and constants you desire, this subroutine must also contain calls to import_array() and/or import_ufunc() depending on which C-API is needed. Forgetting to place these commands will show itself as an ugly segmentation fault (crash) as soon as any C-API subroutine is actually called. It is actually possible to have multiple init{name} functions in a single ﬁle in which case
81

NumPy User Guide, Release 1.11.0
multiple modules will be deﬁned by that ﬁle. However, there are some tricks to get that to work correctly and it is not covered here. A minimal init{name} method looks like:
PyMODINIT_FUNC init{name}(void) {
(void)Py_InitModule({name}, mymethods); import_array(); }
The mymethods must be an array (usually statically declared) of PyMethodDef structures which contain method names, actual C-functions, a variable indicating whether the method uses keyword arguments or not, and docstrings. These are explained in the next section. If you want to add constants to the module, then you store the returned value from Py_InitModule which is a module object. The most general way to add items to the module is to get the module dictionary using PyModule_GetDict(module). With the module dictionary, you can add whatever you like to the module manually. An easier way to add objects to the module is to use one of three additional Python C-API calls that do not require a separate extraction of the module dictionary. These are documented in the Python documentation, but repeated here for convenience: int PyModule_AddObject(PyObject* module, char* name, PyObject* value)
int PyModule_AddIntConstant(PyObject* module, char* name, long value)
int PyModule_AddStringConstant(PyObject* module, char* name, char* value) All three of these functions require the module object (the return value of Py_InitModule). The name is a string that labels the value in the module. Depending on which function is called, the value argument is either a general object (PyModule_AddObject steals a reference to it), an integer constant, or a string constant.
7.1.3 Deﬁning functions
The second argument passed in to the Py_InitModule function is a structure that makes it easy to to deﬁne functions in the module. In the example given above, the mymethods structure would have been deﬁned earlier in the ﬁle (usually right before the init{name} subroutine) to:
static PyMethodDef mymethods[] = { { nokeywordfunc,nokeyword_cfunc, METH_VARARGS, Doc string}, { keywordfunc, keyword_cfunc, METH_VARARGS|METH_KEYWORDS, Doc string}, {NULL, NULL, 0, NULL} /* Sentinel */
}
Each entry in the mymethods array is a PyMethodDef structure containing 1) the Python name, 2) the C-function that implements the function, 3) ﬂags indicating whether or not keywords are accepted for this function, and 4) The docstring for the function. Any number of functions may be deﬁned for a single module by adding more entries to this table. The last entry must be all NULL as shown to act as a sentinel. Python looks for this entry to know that all of the functions for the module have been deﬁned. The last thing that must be done to ﬁnish the extension module is to actually write the code that performs the desired functions. There are two kinds of functions: those that don’t accept keyword arguments, and those that do.
82 Chapter 7. Using Numpy C-API

NumPy User Guide, Release 1.11.0
Functions without keyword arguments
Functions that don’t accept keyword arguments should be written as:
static PyObject* nokeyword_cfunc (PyObject *dummy, PyObject *args) {
/* convert Python arguments */ /* do function */ /* return something */ }
The dummy argument is not used in this context and can be safely ignored. The args argument contains all of the arguments passed in to the function as a tuple. You can do anything you want at this point, but usually the easiest way to manage the input arguments is to call PyArg_ParseTuple (args, format_string, addresses_to_C_variables...) or PyArg_UnpackTuple (tuple, “name” , min, max, ...). A good description of how to use the ﬁrst function is contained in the Python C-API reference manual under section 5.5 (Parsing arguments and building values). You should pay particular attention to the “O&” format which uses converter functions to go between the Python object and the C object. All of the other format functions can be (mostly) thought of as special cases of this general rule. There are several converter functions deﬁned in the NumPy C-API that may be of use. In particular, the PyArray_DescrConverter function is very useful to support arbitrary data-type speciﬁcation. This function transforms any valid data-type Python object into a PyArray_Descr * object. Remember to pass in the address of the C-variables that should be ﬁlled in.
There are lots of examples of how to use PyArg_ParseTuple throughout the NumPy source code. The standard usage is like this:
PyObject *input; PyArray_Descr *dtype; if (!PyArg_ParseTuple(args, "OO&", &input,
PyArray_DescrConverter, &dtype)) return NULL;
It is important to keep in mind that you get a borrowed reference to the object when using the “O” format string. However, the converter functions usually require some form of memory handling. In this example, if the conversion is successful, dtype will hold a new reference to a PyArray_Descr * object, while input will hold a borrowed reference. Therefore, if this conversion were mixed with another conversion (say to an integer) and the data-type conversion was successful but the integer conversion failed, then you would need to release the reference count to the data-type object before returning. A typical way to do this is to set dtype to NULL before calling PyArg_ParseTuple and then use Py_XDECREF on dtype before returning.
After the input arguments are processed, the code that actually does the work is written (likely calling other functions as needed). The ﬁnal step of the C-function is to return something. If an error is encountered then NULL should be returned (making sure an error has actually been set). If nothing should be returned then increment Py_None and return it. If a single object should be returned then it is returned (ensuring that you own a reference to it ﬁrst). If multiple objects should be returned then you need to return a tuple. The Py_BuildValue (format_string, c_variables...) function makes it easy to build tuples of Python objects from C variables. Pay special attention to the difference between ‘N’ and ‘O’ in the format string or you can easily create memory leaks. The ‘O’ format string increments the reference count of the PyObject * C-variable it corresponds to, while the ‘N’ format string steals a reference to the corresponding PyObject * C-variable. You should use ‘N’ if you have already created a reference for the object and just want to give that reference to the tuple. You should use ‘O’ if you only have a borrowed reference to an object and need to create one to provide for the tuple.

7.1. How to extend NumPy

83

NumPy User Guide, Release 1.11.0
Functions with keyword arguments
These functions are very similar to functions without keyword arguments. The only difference is that the function signature is:
static PyObject* keyword_cfunc (PyObject *dummy, PyObject *args, PyObject *kwds) { ... }
The kwds argument holds a Python dictionary whose keys are the names of the keyword arguments and whose values are the corresponding keyword-argument values. This dictionary can be processed however you see ﬁt. The easiest way to handle it, however, is to replace the PyArg_ParseTuple (args, format_string, addresses...) function with a call to PyArg_ParseTupleAndKeywords (args, kwds, format_string, char *kwlist[], addresses...). The kwlist parameter to this function is a NULL -terminated array of strings providing the expected keyword arguments. There should be one string for each entry in the format_string. Using this function will raise a TypeError if invalid keyword arguments are passed in.
For more help on this function please see section 1.8 (Keyword Paramters for Extension Functions) of the Extending and Embedding tutorial in the Python documentation.
Reference counting
The biggest difﬁculty when writing extension modules is reference counting. It is an important reason for the popularity of f2py, weave, Cython, ctypes, etc.... If you mis-handle reference counts you can get problems from memory-leaks to segmentation faults. The only strategy I know of to handle reference counts correctly is blood, sweat, and tears. First, you force it into your head that every Python variable has a reference count. Then, you understand exactly what each function does to the reference count of your objects, so that you can properly use DECREF and INCREF when you need them. Reference counting can really test the amount of patience and diligence you have towards your programming craft. Despite the grim depiction, most cases of reference counting are quite straightforward with the most common difﬁculty being not using DECREF on objects before exiting early from a routine due to some error. In second place, is the common error of not owning the reference on an object that is passed to a function or macro that is going to steal the reference ( e.g. PyTuple_SET_ITEM, and most functions that take PyArray_Descr objects).
Typically you get a new reference to a variable when it is created or is the return value of some function (there are some prominent exceptions, however — such as getting an item out of a tuple or a dictionary). When you own the reference, you are responsible to make sure that Py_DECREF (var) is called when the variable is no longer necessary (and no other function has “stolen” its reference). Also, if you are passing a Python object to a function that will “steal” the reference, then you need to make sure you own it (or use Py_INCREF to get your own reference). You will also encounter the notion of borrowing a reference. A function that borrows a reference does not alter the reference count of the object and does not expect to “hold on “to the reference. It’s just going to use the object temporarily. When you use PyArg_ParseTuple or PyArg_UnpackTuple you receive a borrowed reference to the objects in the tuple and should not alter their reference count inside your function. With practice, you can learn to get reference counting right, but it can be frustrating at ﬁrst.
One common source of reference-count errors is the Py_BuildValue function. Pay careful attention to the difference between the ‘N’ format character and the ‘O’ format character. If you create a new object in your subroutine (such as an output array), and you are passing it back in a tuple of return values, then you should most- likely use the ‘N’ format character in Py_BuildValue. The ‘O’ character will increase the reference count by one. This will leave the caller with two reference counts for a brand-new array. When the variable is deleted and the reference count decremented by one, there will still be that extra reference count, and the array will never be deallocated. You will have a reference-counting induced memory leak. Using the ‘N’ character will avoid this situation as it will return to the caller an object (inside the tuple) with a single reference count.
84 Chapter 7. Using Numpy C-API

NumPy User Guide, Release 1.11.0
7.1.4 Dealing with array objects
Most extension modules for NumPy will need to access the memory for an ndarray object (or one of it’s sub-classes). The easiest way to do this doesn’t require you to know much about the internals of NumPy. The method is to
1. Ensure you are dealing with a well-behaved array (aligned, in machine byte-order and single-segment) of the correct type and number of dimensions.
(a) By converting it from some Python object using PyArray_FromAny or a macro built on it.
(b) By constructing a new ndarray of your desired shape and type using PyArray_NewFromDescr or a simpler macro or function based on it.
2. Get the shape of the array and a pointer to its actual data.
3. Pass the data and shape information on to a subroutine or other section of code that actually performs the computation.
4. If you are writing the algorithm, then I recommend that you use the stride information contained in the array to access the elements of the array (the PyArray_GETPTR macros make this painless). Then, you can relax your requirements so as not to force a single-segment array and the data-copying that might result.
Each of these sub-topics is covered in the following sub-sections.
Converting an arbitrary sequence object
The main routine for obtaining an array from any Python object that can be converted to an array is PyArray_FromAny. This function is very ﬂexible with many input arguments. Several macros make it easier to use the basic function. PyArray_FROM_OTF is arguably the most useful of these macros for the most common uses. It allows you to convert an arbitrary Python object to an array of a speciﬁc builtin data-type ( e.g. ﬂoat), while specifying a particular set of requirements ( e.g. contiguous, aligned, and writeable). The syntax is
PyObject *PyArray_FROM_OTF(PyObject* obj, int typenum, int requirements) Return an ndarray from any Python object, obj, that can be converted to an array. The number of dimensions in the returned array is determined by the object. The desired data-type of the returned array is provided in typenum which should be one of the enumerated types. The requirements for the returned array can be any combination of standard array ﬂags. Each of these arguments is explained in more detail below. You receive a new reference to the array on success. On failure, NULL is returned and an exception is set.
obj
The object can be any Python object convertable to an ndarray. If the object is already (a subclass of) the ndarray that satisﬁes the requirements then a new reference is returned. Otherwise, a new array is constructed. The contents of obj are copied to the new array unless the array interface is used so that data does not have to be copied. Objects that can be converted to an array include: 1) any nested sequence object, 2) any object exposing the array interface, 3) any object with an __array__ method (which should return an ndarray), and 4) any scalar object (becomes a zero-dimensional array). Sub-classes of the ndarray that otherwise ﬁt the requirements will be passed through. If you want to ensure a base-class ndarray, then use NPY_ENSUREARRAY in the requirements ﬂag. A copy is made only if necessary. If you want to guarantee a copy, then pass in NPY_ENSURECOPY to the requirements ﬂag.
typenum
One of the enumerated types or NPY_NOTYPE if the data-type should be determined from the object itself. The C-based names can be used:
NPY_BOOL, NPY_BYTE, NPY_UBYTE, NPY_SHORT, NPY_USHORT, NPY_INT, NPY_UINT, NPY_LONG, NPY_ULONG, NPY_LONGLONG, NPY_ULONGLONG,

7.1. How to extend NumPy

85

NumPy User Guide, Release 1.11.0

NPY_DOUBLE,

NPY_LONGDOUBLE,

NPY_CLONGDOUBLE, NPY_OBJECT.

NPY_CFLOAT,

NPY_CDOUBLE,

Alternatively, the bit-width names can be used as supported on the platform. For example:

NPY_INT8, NPY_INT16, NPY_INT32, NPY_INT64, NPY_UINT8, NPY_UINT16, NPY_UINT32, NPY_UINT64, NPY_FLOAT32, NPY_FLOAT64, NPY_COMPLEX64, NPY_COMPLEX128.

The object will be converted to the desired type only if it can be done without losing precision. Otherwise NULL will be returned and an error raised. Use NPY_FORCECAST in the requirements ﬂag to override this behavior.

requirements

The memory model for an ndarray admits arbitrary strides in each dimension to advance to the next element of the array. Often, however, you need to interface with code that expects a C-contiguous or a Fortran-contiguous memory layout. In addition, an ndarray can be misaligned (the address of an element is not at an integral multiple of the size of the element) which can cause your program to crash (or at least work more slowly) if you try and dereference a pointer into the array data. Both of these problems can be solved by converting the Python object into an array that is more “wellbehaved” for your speciﬁc usage.

The requirements ﬂag allows speciﬁcation of what kind of array is acceptable. If the object passed in does not satisfy this requirements then a copy is made so that thre returned object will satisfy the requirements. these ndarray can use a very generic pointer to memory. This ﬂag allows speciﬁcation of the desired properties of the returned array object. All of the ﬂags are explained in the detailed API chapter. The ﬂags most commonly needed are NPY_ARRAY_IN_ARRAY, NPY_OUT_ARRAY, and NPY_ARRAY_INOUT_ARRAY:

NPY_ARRAY_IN_ARRAY Equivalent to NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED. This combination of ﬂags is useful for arrays that must be in C-contiguous order and aligned. These kinds of arrays are usually input arrays for some algorithm.

NPY_ARRAY_OUT_ARRAY
Equivalent to NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEABLE. This combination of ﬂags is useful to specify an array that is in C-contiguous order, is aligned, and can be written to as well. Such an array is usually returned as output (although normally such output arrays are created from scratch).

NPY_ARRAY_INOUT_ARRAY
Equivalent to NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEABLE | NPY_ARRAY_UPDATEIFCOPY. This combination of ﬂags is useful to specify an array that will be used for both input and output. If a copy is needed, then when the temporary is deleted (by your use of Py_DECREF at the end of the interface routine), the temporary array will be copied back into the original array passed in. Use of the NPY_ARRAY_UPDATEIFCOPY ﬂag requires that the input object is already an array (because other objects cannot be automatically updated in this fashion). If an error occurs use PyArray_DECREF_ERR (obj) on an array with the NPY_ARRAY_UPDATEIFCOPY ﬂag set. This will delete the array without causing the contents to be copied back into the original array.

Other useful ﬂags that can be OR’d as additional requirements are:

NPY_ARRAY_FORCECAST Cast to the desired type, even if it can’t be done without losing information.

NPY_ARRAY_ENSURECOPY Make sure the resulting array is a copy of the original.

86 Chapter 7. Using Numpy C-API

NumPy User Guide, Release 1.11.0
NPY_ARRAY_ENSUREARRAY Make sure the resulting object is an actual ndarray and not a sub- class.
Note: Whether or not an array is byte-swapped is determined by the data-type of the array. Native byte-order arrays are always requested by PyArray_FROM_OTF and so there is no need for a NPY_ARRAY_NOTSWAPPED ﬂag in the requirements argument. There is also no way to get a byte-swapped array from this routine.

Creating a brand-new ndarray
Quite often new arrays must be created from within extension-module code. Perhaps an output array is needed and you don’t want the caller to have to supply it. Perhaps only a temporary array is needed to hold an intermediate calculation. Whatever the need there are simple ways to get an ndarray object of whatever data-type is needed. The most general function for doing this is PyArray_NewFromDescr. All array creation functions go through this heavily re-used code. Because of its ﬂexibility, it can be somewhat confusing to use. As a result, simpler forms exist that are easier to use.
PyObject *PyArray_SimpleNew(int nd, npy_intp* dims, int typenum) This function allocates new memory and places it in an ndarray with nd dimensions whose shape is determined by the array of at least nd items pointed to by dims. The memory for the array is uninitialized (unless typenum is NPY_OBJECT in which case each element in the array is set to NULL). The typenum argument allows speciﬁcation of any of the builtin data-types such as NPY_FLOAT or NPY_LONG. The memory for the array can be set to zero if desired using PyArray_FILLWBYTE (return_object, 0).
PyObject *PyArray_SimpleNewFromData(int nd, npy_intp* dims, int typenum, void* data) Sometimes, you want to wrap memory allocated elsewhere into an ndarray object for downstream use. This routine makes it straightforward to do that. The ﬁrst three arguments are the same as in PyArray_SimpleNew, the ﬁnal argument is a pointer to a block of contiguous memory that the ndarray should use as it’s data-buffer which will be interpreted in C-style contiguous fashion. A new reference to an ndarray is returned, but the ndarray will not own its data. When this ndarray is deallocated, the pointer will not be freed.
You should ensure that the provided memory is not freed while the returned array is in existence. The easiest way to handle this is if data comes from another reference-counted Python object. The reference count on this object should be increased after the pointer is passed in, and the base member of the returned ndarray should point to the Python object that owns the data. Then, when the ndarray is deallocated, the base-member will be DECREF’d appropriately. If you want the memory to be freed as soon as the ndarray is deallocated then simply set the OWNDATA ﬂag on the returned ndarray.
Getting at ndarray memory and accessing elements of the ndarray
If obj is an ndarray (PyArrayObject *), then the data-area of the ndarray is pointed to by the void* pointer PyArray_DATA (obj) or the char* pointer PyArray_BYTES (obj). Remember that (in general) this data-area may not be aligned according to the data-type, it may represent byte-swapped data, and/or it may not be writeable. If the data area is aligned and in native byte-order, then how to get at a speciﬁc element of the array is determined only by the array of npy_intp variables, PyArray_STRIDES (obj). In particular, this c-array of integers shows how many bytes must be added to the current element pointer to get to the next element in each dimension. For arrays less than 4-dimensions there are PyArray_GETPTR{k} (obj, ...) macros where {k} is the integer 1, 2, 3, or 4 that make using the array strides easier. The arguments .... represent {k} non- negative integer indices into the array. For example, suppose E is a 3-dimensional ndarray. A (void*) pointer to the element E[i,j,k] is obtained as PyArray_GETPTR3 (E, i, j, k).
As explained previously, C-style contiguous arrays and Fortran-style contiguous arrays have particular striding patterns. Two array ﬂags (NPY_C_CONTIGUOUS and :cdata‘NPY_F_CONTIGUOUS‘) indicate whether or not the striding pattern of a particular array matches the C-style contiguous or Fortran-style contiguous or neither. Whether or not the striding pattern matches a standard C or Fortran one can be tested Using PyArray_ISCONTIGUOUS (obj)

7.1. How to extend NumPy

87

NumPy User Guide, Release 1.11.0
and PyArray_ISFORTRAN (obj) respectively. Most third-party libraries expect contiguous arrays. But, often it is not difﬁcult to support general-purpose striding. I encourage you to use the striding information in your own code whenever possible, and reserve single-segment requirements for wrapping third-party code. Using the striding information provided with the ndarray rather than requiring a contiguous striding reduces copying that otherwise must be made.
7.1.5 Example
The following example shows how you might write a wrapper that accepts two input arguments (that will be converted to an array) and an output argument (that must be an array). The function returns None and updates the output array.
static PyObject * example_wrapper(PyObject *dummy, PyObject *args) {
PyObject *arg1=NULL, *arg2=NULL, *out=NULL; PyObject *arr1=NULL, *arr2=NULL, *oarr=NULL;
if (!PyArg_ParseTuple(args, "OOO!", &arg1, &arg2, &PyArray_Type, &out)) return NULL;
arr1 = PyArray_FROM_OTF(arg1, NPY_DOUBLE, NPY_IN_ARRAY); if (arr1 == NULL) return NULL; arr2 = PyArray_FROM_OTF(arg2, NPY_DOUBLE, NPY_IN_ARRAY); if (arr2 == NULL) goto fail; oarr = PyArray_FROM_OTF(out, NPY_DOUBLE, NPY_INOUT_ARRAY); if (oarr == NULL) goto fail;
/* code that makes use of arguments */ /* You will probably need at least
nd = PyArray_NDIM(<..>) -- number of dimensions dims = PyArray_DIMS(<..>) -- npy_intp array of length nd
showing length in each dim. dptr = (double *)PyArray_DATA(<..>) -- pointer to data.
If an error occurs goto fail. */
Py_DECREF(arr1); Py_DECREF(arr2); Py_DECREF(oarr); Py_INCREF(Py_None); return Py_None;
fail: Py_XDECREF(arr1); Py_XDECREF(arr2); PyArray_XDECREF_ERR(oarr); return NULL;
}
7.2 Using Python as glue
There is no conversation more boring than the one where everybody agrees.
88 Chapter 7. Using Numpy C-API

NumPy User Guide, Release 1.11.0
— Michel de Montaigne
Duct tape is like the force. It has a light side, and a dark side, and it holds the universe together. — Carl Zwanzig
Many people like to say that Python is a fantastic glue language. Hopefully, this Chapter will convince you that this is true. The ﬁrst adopters of Python for science were typically people who used it to glue together large application codes running on super-computers. Not only was it much nicer to code in Python than in a shell script or Perl, in addition, the ability to easily extend Python made it relatively easy to create new classes and types speciﬁcally adapted to the problems being solved. From the interactions of these early contributors, Numeric emerged as an array-like object that could be used to pass data between these applications.
As Numeric has matured and developed into NumPy, people have been able to write more code directly in NumPy. Often this code is fast-enough for production use, but there are still times that there is a need to access compiled code. Either to get that last bit of efﬁciency out of the algorithm or to make it easier to access widely-available codes written in C/C++ or Fortran.
This chapter will review many of the tools that are available for the purpose of accessing code written in other compiled languages. There are many resources available for learning to call other compiled libraries from Python and the purpose of this Chapter is not to make you an expert. The main goal is to make you aware of some of the possibilities so that you will know what to “Google” in order to learn more.
7.2.1 Calling other compiled libraries from Python
While Python is a great language and a pleasure to code in, its dynamic nature results in overhead that can cause some code ( i.e. raw computations inside of for loops) to be up 10-100 times slower than equivalent code written in a static compiled language. In addition, it can cause memory usage to be larger than necessary as temporary arrays are created and destroyed during computation. For many types of computing needs, the extra slow-down and memory consumption can often not be spared (at least for time- or memory- critical portions of your code). Therefore one of the most common needs is to call out from Python code to a fast, machine-code routine (e.g. compiled using C/C++ or Fortran). The fact that this is relatively easy to do is a big reason why Python is such an excellent high-level language for scientiﬁc and engineering programming.
Their are two basic approaches to calling compiled code: writing an extension module that is then imported to Python using the import command, or calling a shared-library subroutine directly from Python using the ctypes module. Writing an extension module is the most common method.
Warning: Calling C-code from Python can result in Python crashes if you are not careful. None of the approaches in this chapter are immune. You have to know something about the way data is handled by both NumPy and by the third-party library being used.

7.2.2 Hand-generated wrappers
Extension modules were discussed in Writing an extension module. The most basic way to interface with compiled code is to write an extension module and construct a module method that calls the compiled code. For improved readability, your method should take advantage of the PyArg_ParseTuple call to convert between Python objects and C data-types. For standard C data-types there is probably already a built-in converter. For others you may need to write your own converter and use the "O&" format string which allows you to specify a function that will be used to perform the conversion from the Python object to whatever C-structures are needed.

7.2. Using Python as glue

89

NumPy User Guide, Release 1.11.0
Once the conversions to the appropriate C-structures and C data-types have been performed, the next step in the wrapper is to call the underlying function. This is straightforward if the underlying function is in C or C++. However, in order to call Fortran code you must be familiar with how Fortran subroutines are called from C/C++ using your compiler and platform. This can vary somewhat platforms and compilers (which is another reason f2py makes life much simpler for interfacing Fortran code) but generally involves underscore mangling of the name and the fact that all variables are passed by reference (i.e. all arguments are pointers). The advantage of the hand-generated wrapper is that you have complete control over how the C-library gets used and called which can lead to a lean and tight interface with minimal over-head. The disadvantage is that you have to write, debug, and maintain C-code, although most of it can be adapted using the time-honored technique of “cuttingpasting-and-modifying” from other extension modules. Because, the procedure of calling out to additional C-code is fairly regimented, code-generation procedures have been developed to make this process easier. One of these codegeneration techniques is distributed with NumPy and allows easy integration with Fortran and (simple) C code. This package, f2py, will be covered brieﬂy in the next section.
7.2.3 f2py
F2py allows you to automatically construct an extension module that interfaces to routines in Fortran 77/90/95 code. It has the ability to parse Fortran 77/90/95 code and automatically generate Python signatures for the subroutines it encounters, or you can guide how the subroutine interfaces with Python by constructing an interface-deﬁnition-ﬁle (or modifying the f2py-produced one).
Creating source for a basic extension module
Probably the easiest way to introduce f2py is to offer a simple example. Here is one of the subroutines contained in a ﬁle named add.f:
C SUBROUTINE ZADD(A,B,C,N)
C DOUBLE COMPLEX A(*) DOUBLE COMPLEX B(*) DOUBLE COMPLEX C(*) INTEGER N DO 20 J = 1, N C(J) = A(J)+B(J)
20 CONTINUE END
This routine simply adds the elements in two contiguous arrays and places the result in a third. The memory for all three arrays must be provided by the calling routine. A very basic interface to this routine can be automatically generated by f2py:
f2py -m add add.f
You should be able to run this command assuming your search-path is set-up properly. This command will produce an extension module named addmodule.c in the current directory. This extension module can now be compiled and used from Python just like any other extension module.
Creating a compiled extension module
You can also get f2py to compile add.f and also compile its produced extension module leaving only a shared-library extension ﬁle that can be imported from Python:
90 Chapter 7. Using Numpy C-API

NumPy User Guide, Release 1.11.0

f2py -c -m add add.f
This command leaves a ﬁle named add.{ext} in the current directory (where {ext} is the appropriate extension for a python extension module on your platform — so, pyd, etc. ). This module may then be imported from Python. It will contain a method for each subroutine in add (zadd, cadd, dadd, sadd). The docstring of each method contains information about how the module method may be called:
>>> import add >>> print add.zadd.__doc__ zadd - Function signature:
zadd(a,b,c,n) Required arguments:
a : input rank-1 array('D') with bounds (*) b : input rank-1 array('D') with bounds (*) c : input rank-1 array('D') with bounds (*) n : input int
Improving the basic interface
The default interface is a very literal translation of the fortran code into Python. The Fortran array arguments must now be NumPy arrays and the integer argument should be an integer. The interface will attempt to convert all arguments to their required types (and shapes) and issue an error if unsuccessful. However, because it knows nothing about the semantics of the arguments (such that C is an output and n should really match the array sizes), it is possible to abuse this function in ways that can cause Python to crash. For example:
>>> add.zadd([1,2,3], [1,2], [3,4], 1000)
will cause a program crash on most systems. Under the covers, the lists are being converted to proper arrays but then the underlying add loop is told to cycle way beyond the borders of the allocated memory.
In order to improve the interface, directives should be provided. This is accomplished by constructing an interface deﬁnition ﬁle. It is usually best to start from the interface ﬁle that f2py can produce (where it gets its default behavior from). To get f2py to generate the interface ﬁle use the -h option:
f2py -h add.pyf -m add add.f
This command leaves the ﬁle add.pyf in the current directory. The section of this ﬁle corresponding to zadd is:
subroutine zadd(a,b,c,n) ! in :add:add.f double complex dimension(*) :: a double complex dimension(*) :: b double complex dimension(*) :: c integer :: n
end subroutine zadd
By placing intent directives and checking code, the interface can be cleaned up quite a bit until the Python module method is both easier to use and more robust.
subroutine zadd(a,b,c,n) ! in :add:add.f double complex dimension(n) :: a double complex dimension(n) :: b double complex intent(out),dimension(n) :: c integer intent(hide),depend(a) :: n=len(a)
end subroutine zadd
The intent directive, intent(out) is used to tell f2py that c is an output variable and should be created by the interface before being passed to the underlying code. The intent(hide) directive tells f2py to not allow the user to specify the

7.2. Using Python as glue

91

NumPy User Guide, Release 1.11.0
variable, n, but instead to get it from the size of a. The depend( a ) directive is necessary to tell f2py that the value of n depends on the input a (so that it won’t try to create the variable n until the variable a is created). After modifying add.pyf, the new python module ﬁle can be generated by compiling both add.f95 and add.pyf:
f2py -c add.pyf add.f95
The new interface has docstring:
>>> import add >>> print add.zadd.__doc__ zadd - Function signature:
c = zadd(a,b) Required arguments:
a : input rank-1 array('D') with bounds (n) b : input rank-1 array('D') with bounds (n) Return objects: c : rank-1 array('D') with bounds (n)
Now, the function can be called in a much more robust way:
>>> add.zadd([1,2,3],[4,5,6]) array([ 5.+0.j, 7.+0.j, 9.+0.j])
Notice the automatic conversion to the correct format that occurred.
Inserting directives in Fortran source
The nice interface can also be generated automatically by placing the variable directives as special comments in the original fortran code. Thus, if I modify the source code to contain: C
SUBROUTINE ZADD(A,B,C,N) C CF2PY INTENT(OUT) :: C CF2PY INTENT(HIDE) :: N CF2PY DOUBLE COMPLEX :: A(N) CF2PY DOUBLE COMPLEX :: B(N) CF2PY DOUBLE COMPLEX :: C(N)
DOUBLE COMPLEX A(*) DOUBLE COMPLEX B(*) DOUBLE COMPLEX C(*) INTEGER N DO 20 J = 1, N
C(J) = A(J) + B(J) 20 CONTINUE
END
Then, I can compile the extension module using:
f2py -c -m add add.f
The resulting signature for the function add.zadd is exactly the same one that was created previously. If the original source code had contained A(N) instead of A(*) and so forth with B and C, then I could obtain (nearly) the same interface simply by placing the INTENT(OUT) :: C comment line in the source code. The only difference is that N would be an optional input that would default to the length of A.
92 Chapter 7. Using Numpy C-API

NumPy User Guide, Release 1.11.0

A ﬁltering example
For comparison with the other methods to be discussed. Here is another example of a function that ﬁlters a twodimensional array of double precision ﬂoating-point numbers using a ﬁxed averaging ﬁlter. The advantage of using Fortran to index into multi-dimensional arrays should be clear from this example.
SUBROUTINE DFILTER2D(A,B,M,N) C
DOUBLE PRECISION A(M,N) DOUBLE PRECISION B(M,N) INTEGER N, M CF2PY INTENT(OUT) :: B CF2PY INTENT(HIDE) :: N CF2PY INTENT(HIDE) :: M DO 20 I = 2,M-1
DO 40 J=2,N-1 B(I,J) = A(I,J) +
$ (A(I-1,J)+A(I+1,J) + $ A(I,J-1)+A(I,J+1) )*0.5D0 + $ (A(I-1,J-1) + A(I-1,J+1) + $ A(I+1,J-1) + A(I+1,J+1))*0.25D0 40 CONTINUE 20 CONTINUE
END
This code can be compiled and linked into an extension module named ﬁlter using:
f2py -c -m filter filter.f
This will produce an extension module named ﬁlter.so in the current directory with a method named dﬁlter2d that returns a ﬁltered version of the input.
Calling f2py from Python
The f2py program is written in Python and can be run from inside your code to compile Fortran code at runtime, as follows:
from numpy import f2py with open("add.f") as sourcefile:
sourcecode = sourcefile.read() f2py.compile(sourcecode, modulename='add') import add
The source string can be any valid Fortran code. If you want to save the extension-module source code then a suitable ﬁle-name can be provided by the source_fn keyword to the compile function.
Automatic extension module generation
If you want to distribute your f2py extension module, then you only need to include the .pyf ﬁle and the Fortran code. The distutils extensions in NumPy allow you to deﬁne an extension module entirely in terms of this interface ﬁle. A valid setup.py ﬁle allowing distribution of the add.f module (as part of the package f2py_examples so that it would be loaded as f2py_examples.add) is:
def configuration(parent_package='', top_path=None) from numpy.distutils.misc_util import Configuration config = Configuration('f2py_examples',parent_package, top_path) config.add_extension('add', sources=['add.pyf','add.f'])

7.2. Using Python as glue

93

NumPy User Guide, Release 1.11.0
return config
if __name__ == '__main__': from numpy.distutils.core import setup setup(**configuration(top_path='').todict())
Installation of the new package is easy using:
python setup.py install
assuming you have the proper permissions to write to the main site- packages directory for the version of Python you are using. For the resulting package to work, you need to create a ﬁle named __init__.py (in the same directory as add.pyf). Notice the extension module is deﬁned entirely in terms of the add.pyf and add.f ﬁles. The conversion of the .pyf ﬁle to a .c ﬁle is handled by numpy.disutils.
Conclusion
The interface deﬁnition ﬁle (.pyf) is how you can ﬁne-tune the interface between Python and Fortran. There is decent documentation for f2py found in the numpy/f2py/docs directory where-ever NumPy is installed on your system (usually under site-packages). There is also more information on using f2py (including how to use it to wrap C codes) at http://www.scipy.org/Cookbook under the “Using NumPy with Other Languages” heading. The f2py method of linking compiled code is currently the most sophisticated and integrated approach. It allows clean separation of Python with compiled code while still allowing for separate distribution of the extension module. The only draw-back is that it requires the existence of a Fortran compiler in order for a user to install the code. However, with the existence of the free-compilers g77, gfortran, and g95, as well as high-quality commerical compilers, this restriction is not particularly onerous. In my opinion, Fortran is still the easiest way to write fast and clear code for scientiﬁc computing. It handles complex numbers, and multi-dimensional indexing in the most straightforward way. Be aware, however, that some Fortran compilers will not be able to optimize code as well as good hand- written C-code.
7.2.4 Cython
Cython is a compiler for a Python dialect that adds (optional) static typing for speed, and allows mixing C or C++ code into your modules. It produces C or C++ extensions that can be compiled and imported in Python code. If you are writing an extension module that will include quite a bit of your own algorithmic code as well, then Cython is a good match. Among its features is the ability to easily and quickly work with multidimensional arrays. Notice that Cython is an extension-module generator only. Unlike f2py, it includes no automatic facility for compiling and linking the extension module (which must be done in the usual fashion). It does provide a modiﬁed distutils class called build_ext which lets you build an extension module from a .pyx source. Thus, you could write in a setup.py ﬁle:
from Cython.Distutils import build_ext from distutils.extension import Extension from distutils.core import setup import numpy
setup(name='mine', description='Nothing', ext_modules=[Extension('filter', ['filter.pyx'], include_dirs=[numpy.get_include()])], cmdclass = {'build_ext':build_ext})
Adding the NumPy include directory is, of course, only necessary if you are using NumPy arrays in the extension module (which is what we assume you are using Cython for). The distutils extensions in NumPy also include support
94 Chapter 7. Using Numpy C-API

NumPy User Guide, Release 1.11.0
for automatically producing the extension-module and linking it from a .pyx ﬁle. It works so that if the user does not have Cython installed, then it looks for a ﬁle with the same ﬁle-name but a .c extension which it then uses instead of trying to produce the .c ﬁle again.
If you just use Cython to compile a standard Python module, then you will get a C extension module that typically runs a bit faster than the equivalent Python module. Further speed increases can be gained by using the cdef keyword to statically deﬁne C variables.
Let’s look at two examples we’ve seen before to see how they might be implemented using Cython. These examples were compiled into extension modules using Cython 0.21.1.
Complex addition in Cython
Here is part of a Cython module named add.pyx which implements the complex addition functions we previously implemented using f2py:
cimport cython cimport numpy as np import numpy as np
# We need to initialize NumPy. np.import_array()
#@cython.boundscheck(False) def zadd(in1, in2):
cdef double complex[:] a = in1.ravel() cdef double complex[:] b = in2.ravel()
out = np.empty(a.shape[0], np.complex64) cdef double complex[:] c = out.ravel()
for i in range(c.shape[0]): c[i].real = a[i].real + b[i].real c[i].imag = a[i].imag + b[i].imag
return out
This module shows use of the cimport statement to load the deﬁnitions from the numpy.pxd header that ships with Cython. It looks like NumPy is imported twice; cimport only makes the NumPy C-API available, while the regular import causes a Python-style import at runtime and makes it possible to call into the familiar NumPy Python API.
The example also demonstrates Cython’s “typed memoryviews”, which are like NumPy arrays at the C level, in the sense that they are shaped and strided arrays that know their own extent (unlike a C array addressed through a bare pointer). The syntax double complex[:] denotes a one-dimensional array (vector) of doubles, with arbitrary strides. A contiguous array of ints would be int[::1], while a matrix of ﬂoats would be float[:, :].
Shown commented is the cython.boundscheck decorator, which turns bounds-checking for memory view accesses on or off on a per-function basis. We can use this to further speed up our code, at the expense of safety (or a manual check prior to entering the loop).
Other than the view syntax, the function is immediately readable to a Python programmer. Static typing of the variable i is implicit. Instead of the view syntax, we could also have used Cython’s special NumPy array syntax, but the view syntax is preferred.

7.2. Using Python as glue

95

NumPy User Guide, Release 1.11.0
Image ﬁlter in Cython
The two-dimensional example we created using Fortran is just as easy to write in Cython: cimport numpy as np import numpy as np
np.import_array()
def filter(img): cdef double[:, :] a = np.asarray(img, dtype=np.double) out = np.zeros(img.shape, dtype=np.double) cdef double[:, ::1] b = out
cdef np.npy_intp i, j
for i in range(1, a.shape[0] - 1): for j in range(1, a.shape[1] - 1): b[i, j] = (a[i, j] + .5 * ( a[i-1, j] + a[i+1, j] + a[i, j-1] + a[i, j+1]) + .25 * ( a[i-1, j-1] + a[i-1, j+1] + a[i+1, j-1] + a[i+1, j+1]))
return out
This 2-d averaging ﬁlter runs quickly because the loop is in C and the pointer computations are done only as needed. If the code above is compiled as a module image, then a 2-d image, img, can be ﬁltered using this code very quickly using: import image out = image.filter(img)
Regarding the code, two things are of note: ﬁrstly, it is impossible to return a memory view to Python. Instead, a NumPy array out is ﬁrst created, and then a view b onto this array is used for the computation. Secondly, the view b is typed double[:, ::1]. This means 2-d array with contiguous rows, i.e., C matrix order. Specifying the order explicitly can speed up some algorithms since they can skip stride computations.
Conclusion
Cython is the extension mechanism of choice for several scientiﬁc Python libraries, including Scipy, Pandas, SAGE, scikit-image and scikit-learn, as well as the XML processing library LXML. The language and compiler are wellmaintained. There are several disadvantages of using Cython:
1. When coding custom algorithms, and sometimes when wrapping existing C libraries, some familiarity with C is required. In particular, when using C memory management (malloc and friends), it’s easy to introduce memory leaks. However, just compiling a Python module renamed to .pyx can already speed it up, and adding a few type declarations can give dramatic speedups in some code.
2. It is easy to lose a clean separation between Python and C which makes re-using your C-code for other nonPython-related projects more difﬁcult.
3. The C-code generated by Cython is hard to read and modify (and typically compiles with annoying but harmless warnings).
96 Chapter 7. Using Numpy C-API

